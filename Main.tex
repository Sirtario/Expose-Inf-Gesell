\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}

\title{Stand der Forschung \\[1ex] \large Thema: Welche Methodiken sind möglich bzw. werden angewandt, um die Erzeugung ethisch fragwürdiger Inhalte durch IGAI zu verhindern?}
\date{16.04.2023}
\author{Ronja Drechsler, \and Dominic Fitter, \and Michel Hecker, \and Khaldon Kassem, \and Phillip Eckstein}

\begin{document}

\maketitle
\tableofcontents
\newpage
\chapter{Einleitung}
In den letzten Jahren haben Fortschritte in der Künstlichen Intelligenz (KI) dazu geführt, dass Bildgenerierende KI-Systeme immer 
leistungsfähiger geworden sind. Diese Systeme können nun hochauflösende Bilder in beispiellosem Detail generieren und haben das Potenzial,
 in vielen Branchen eingesetzt zu werden, einschließlich der Unterhaltungsindustrie, des Einzelhandels und des Gesundheitswesens. 
 Allerdings haben diese Fortschritte auch einige Bedenken hervorgerufen, insbesondere in Bezug auf die ethischen Implikationen, 
 die damit verbunden sind.
 in wichtiger Aspekt, der bei der Verwendung von bildgenerierenden KI-Systemen berücksichtigt werden muss, ist die Möglichkeit, 
 dass ethisch fragwürdige Inhalte generiert werden können. Es gibt ein wachsendes Bewusstsein für die Notwendigkeit, 
 sicherzustellen, dass KI-Systeme keine rassistischen, sexistischen oder anderweitig diskriminierenden Inhalte generieren. Diese 
 Herausforderung stellt eine wichtige Forschungsfrage dar, die untersucht werden muss, um sicherzustellen, dass die Verwendung von
  bildgenerierenden KI-Systemen ethisch vertretbar bleibt.

 \section{Motivation und Relevanz}
 Die Entwicklung von AI-Kunst reicht zurück bis in die 1960er Jahre\cite{Garcia}, aber erst in den letzten Jahren wurden 
 diese KIs durch die Veröffentlichungen von bahnbrechenden Projekten wie DALL-E (01/21), Midjourney (03/22) oder Stable Diffusion 
 (08/22) der Allgemeinheit zugänglich gemacht. Auslotungen der Leistungsfähigkeit dieser KIs zeigen jedoch auch auf, 
 dass sie sexistische oder rassistische Verzerrungen aufweisen \cite{Schmidt}und für moralisch fragwürdige Zwecke missbraucht werden 
 können, z. B. bei der Erstellung von Deepfakes, pornografischen oder gewalttätigen Inhalten.\cite{Hadero}

 Angesichts dessen haben u. a. die Entwickler von Midjourney reagiert und das Verwenden von Wörtern mit Bezug zur 
 Thematik menschlicher Fortpflanzungssysteme verboten \cite{Heikkilae} und ihre kostenlose Testversion eingestellt. 
 \cite{NelsonMidjourney} Doch es stellt sich die 
 Frage, wie Anbieter und Entwickler solcher KIs gegen diese schädliche Verwendung ihrer Technologie vorgehen können.
 
 Die vorliegende Arbeit wird sich mit dieser Frage auseinandersetzen und untersuchen, welche Möglichkeiten es gibt, um KIs vor 
 Missbrauch zu schützen. Sie wird diskutieren, wie die Anbieter und Entwickler dieser Technologie verantwortungsbewusst 
 handeln können, um sicherzustellen, dass ihre KIs nur für ethisch vertretbare Zwecke eingesetzt werden. Das Ziel ist es, 
 Wege aufzuzeigen, wie KI-Entwickler dazu beitragen können, die positiven Auswirkungen der Technologie zu maximieren, 
 während sie gleichzeitig die negativen Auswirkungen minimieren.
 \section{Forschungsfrage}
 In dieser Arbeit soll die Frage geklärt werden welche Methodiken möglich sind bzw. angewandt werden, um die Erzeugung ethisch 
 fragwürdiger Inhalte durch IGAI zu verhindern?
 \include{StandForschung.tex}
\include{Methodik.tex}
\chapter{Definitionen}
Wie in der Methodik erwähnt, werden im Folgenden die Begriffe der bildgenerienden KI und der Ethik im Bezug auf dieser für diese Arbeit definiert. 
Durch diese Vorgehensweise soll vermieden werden, dass diese Thematiken bei kommenden Durchführung immer neu angesprochen werden müssen und sich die Arbeit so auf klare und einheitliche Definitionen der Begriffe stützen kann.

\section{Definitionen}
Wie in der Methodik erwähnt, werden im Folgenden die Begriffe der bildgenerienden KI und der Ethik im Bezug auf dieser für diese Arbeit definiert. 
Durch diese Vorgehensweise soll vermieden werden, dass diese Thematiken bei kommenden Durchführung immer neu angesprochen werden müssen und sich die Arbeit so auf klare und einheitliche Definitionen der Begriffe stützen kann.

\subsection{Ethik}\label{subsection:ethicsdefinition}
\paragraph{was Herr Müller sich gewünscht hatte:} 
das wird ja von unterschiedlichen Ebenen beschlossen (un gibt Resolution, EU setzt das in Richtlinien um, de muss das in nationales Recht übertragen) wir haben uns auf EU eben geeinigt, er möchte den Prozess mal kurz haben, warum wir uns darauf festgelegt haben (ich nehme da dann an, weil das noch nicht in nationales recht übernommen wurde?) und dann halt noch mal was da dann explizit drin steht (das müsste von khaldon schon da sein, da hätte sich Herr Müller aber nen eigenen text gewünscht statt den text zu Ctrl c + V)


Um als vertrauenswürdig betrachtet zu werden, muss Künstliche Intelligenz (KI) während des gesamten Lebenszyklus des Systems drei grundlegende Komponenten erfüllen: a) sie muss rechtmäßig sein, b) sie muss ethisch sein und c) sie muss robust sein.

Im Rahmen der Entwicklung, Bereitstellung und Nutzung von KI-Systemen werden in Kapitel I der \glqq ETHIK-LEITLINIEN FÜR EINE VERTRAUENSWÜRDIGE KI\grqq{} der EU-Kommission folgende ethische Grundsätze identifiziert:

\glqq 1. Die Entwicklung, Einführung und Nutzung von KI-Systemen muss so erfolgen, dass die folgenden ethischen Grundsätze eingehalten werden: \textit{Achtung der menschlichen Autonomie, Schadensverhütung, Fairness und Erklärbarkeit}. Die möglichen Spannungen zwischen diesen Grundsätzen müssen zur Kenntnis genommen und gelöst werden. 
2. Besondere Berücksichtigung von Situationen, in denen besonders schutzbedürftige Gruppen wie Kinder, Menschen mit Behinderungen und andere betroffen sind, die schon in der Vergangenheit Benachteiligung erfahren haben oder die einem besonders hohen Exklusionsrisiko ausgesetzt sind. Gleiches gilt für Situationen, die sich durch ungleiche Macht- oder Informationsverteilung auszeichnen, etwa zwischen Arbeitgebern und Arbeitnehmern oder Unternehmen und Verbrauchern.
3. Es gilt anzuerkennen und zu berücksichtigen, dass KI-Systeme dem Einzelnen und der Gesellschaft zwar einen erheblichen Nutzen bringen, gleichzeitig jedoch bestimmte Risiken bergen und möglicherweise negative, mitunter schwer absehbare, erkennbare oder messbare Auswirkungen (z. B. im Hinblick auf Demokratie, Rechtsstaatlichkeit, Verteilungsgerechtigkeit oder den menschlichen Geist als solchen) haben können. Zur Abwendung dieser Gefahren müssen gegebenenfalls angemessene und in Anbetracht der Höhe des Risikos verhältnismäßige Maßnahmen getroffen werden.\grqq{}

Wie in Kapitel II beschrieben, gibt es sieben Anforderungen an KI-Systeme, um eine vertrauenswürdige KI freizugeben:

\glqq 1. Es muss gewährleistet sein, dass die Entwicklung, Einführung und Nutzung von KI-Systemen die Anforderungen an vertrauenswürdige KI erfüllen: 1) Vorrang menschlichen Handelns und menschliche Aufsicht, 2) technische Robustheit und Sicherheit, 3) Schutz der Privatsphäre und Datenqualitätsmanagement, 4) Transparenz, 5) Vielfalt, Nichtdiskriminierung und Fairness, 6) gesellschaftliches und ökologisches Wohlergehen sowie 7) Rechenschaftspflicht. 
2. Berücksichtigung technischer und nichttechnischer Methoden, um die Umsetzung dieser Anforderungen sicherzustellen. 
3. Förderung von Forschung und Innovation als Beitrag zur Bewertung von KI-Systemen und zur weiteren Erfüllung der Anforderungen; Bekanntmachung von Ergebnissen und offenen Fragen in der breiten Öffentlichkeit sowie systematische Ausbildung einer neuen Generation von Experten auf dem Gebiet der KI-Ethik. 
4. Klare und proaktive Informationsübermittlung an betroffene Kreise über die Fähigkeiten und Grenzen der KI-Systeme, die realistische Erwartungen ermöglichen, sowie über die Art und Weise der Implementierung der Anforderungen. Für die Anwender muss klar erkennbar sein, dass sie es mit einem KI-System zu tun haben. 
5. Möglichkeit der Rückverfolgbarkeit und Nachprüfbarkeit von KI-Systemen, insbesondere in kritischen Zusammenhängen oder Situationen. 
6. Beteiligung der Interessenträger während des gesamten Lebenszyklus des KI-Systems. Schulungs- und Ausbildungsförderung mit dem Ziel, allen Interessenträgern Kompetenzen auf dem Gebiet der vertrauenswürdigen KI zu vermitteln. 
7. Zwischen den verschiedenen Grundsätzen und Anforderungen können möglicherweise wesentliche Spannungen auftreten. Diesbezügliche Kompromisse und Lösungen müssen kontinuierlich ermittelt, bewertet, dokumentiert und mitgeteilt werden.\grqq{}


\subsection{AI}
Im Gegensatz zu bisherigen AIs können den generative artifical intelligence (GAI) neue Daten generieren, anstatt wie bisher Daten zu analysieren oder basierend auf gegebenen Daten zu handeln. Außerdem werden ihnen zum Trainieren enorme Datenmengen, 
wie der komplette Inhalt von Wikipedia, zur Verfügung gestellt. Daher sind diese AIs erst mit steigender Rechenkapazität möglich geworden. GAIs modellieren Netze aus miteinander verbundenen Daten in verschiedenen multimedialen Formaten. Demnach 
bestünden beispielsweise Verbindungen zwischen dem Wort Panda, ein Bild von einem Panda und einem Video von einem Panda. Dadurch können GAIs ein beliebiges Eingabeformat in ein beliebiges Ausgabeformat übersetzen. Thema dieser Arbeit sind image generating 
artifical intelligence (IGAI) oder auch text-to-image AIs, eine Unterkategorie von GAIs. Diese AIs können nach Eingabe von natürlichsprachlichem Text Bilder erzeugen.


Um GAIs zu trainieren, werden drei Arten von Machine Learning verwendet: überwachtes Lernen, unüberwachtes Lernen und verstärktes Lernen. 
Beim überwachten Lernen werden gekennzeichnete Datensätze verwendet, um Vorhersagen zu treffen. Die gekennzeichneten Daten werden als Trainingsdaten verwendet und nach dem Ausführen des Algorithmus wird überprüft, ob die getroffene Vorhersage der Kennzeichnung entspricht.
Beim unüberwachten Lernen wird dem Algorithmus nicht gekennzeichnete Daten zugeführt. Hierbei wird nicht versucht eine Vorhersage zu treffen, sondern Schlussfolgerungen oder Zusammenhänge zwischen den Daten zu finden.
Beim verstärkten Lernen wird ein System aus Belohnung und Strafen verwendet. Macht der Algorithmus wenige Fehler ist die Belohnung groß und die Strafe klein, macht der Algorithmus viele Fehler ist die Belohnung klein und die Strafe groß. Dadurch kann das Finden des 
optimalen Verhaltens automatisiert werden.

\chapter{Durchführung}
Verschiedene Formen der Voreignenommenheit von AI lassen sich durch unterschiedliche Methoden verhindern. Diese lassen sich allgemein in 
Technische und Nicht-Technische Methoden unterteilen. Nachfolgen werden verschiedene Methoden erläutert.
\section{Nicht-Technische Methoden}
Nicht-Technische Methoden sind solche, die nicht primär auf Technische Lösungen setzen.

\subsection{AGBs und TOS}
Allgemeine Geschäftsbedingungen (AGBs) oder auch Terms of Service (TOS) etablieren den rechtlichen Rahmen für die Nutzung von IGAIs. Dadurch können sie dazu beitragen, den Missbrauch solcher Systeme einzudämmen. 
Allerdings sind AGBs allein nicht hinreichend, um den Missbrauch von IGAIs gänzlich zu verhindern, da sie keine aktive Nutzungsbeschränkung bieten. Wesentliche Aspekte könnten sein:
\begin{itemize}
    \item Festlegung des Nutzungszwecks: AGBs oder TOS sollten explizit den beabsichtigten Nutzungszweck der KI definieren. Dadurch wird ein Rahmen für akzeptable Aktivitäten abgesteckt und unerwünschte Nutzungsformen ausgeschlossen.
    \item Untersagung unzulässiger Aktivitäten: AGBs sollten eine Liste von Aktivitäten enthalten, die als Missbrauch der KI gelten und somit untersagt sind.
    \item Haftungsausschluss: Ein Haftungsausschluss in den AGBs ist bedeutsam, um zu verdeutlichen, dass der Anbieter der KI nicht für den Missbrauch der Technologie durch die Nutzer verantwortlich gemacht werden kann. Dadurch sollen potenzielle rechtliche Konsequenzen für den Anbieter eingeschränkt werden
    \item Festlegung von Nutzungsregeln: Die AGBs  können spezifische Regeln für die Nutzer der KI enthalten, um den Missbrauch einzudämmen. Dies könnte beispielsweise die Verpflichtung zur Registrierung, die Verwendung sicherer Zugangsdaten oder die Einhaltung ethischer Richtlinien umfassen.
    \item Überwachung und Sanktionen: AGBs sollten auch klare Informationen darüber bereitstellen, wie die Nutzung der KI überwacht wird und welche Sanktionen im Falle von Missbrauch verhängt werden können. Dies kann beispielsweise die Möglichkeit zur Überprüfung von Nutzeraktivitäten, die Sperrung von Konten oder die Meldung rechtswidrigen Verhaltens umfassen.
\end{itemize}

\subsection{Zusammenarbeit und Feedback}
Benutzerfeedback und Engagement beinhalten das aktive Einbeziehen von Benutzern während des Entwicklungsprozesses und die Integration ihrer Perspektiven, Bedenken und Werte in die Gestaltung und Entscheidungsfindung von KI-Systemen. Durch die Berücksichtigung des Benutzerfeedbacks können KI-Entwickler wertvolle Einblicke in die potenziellen ethischen Implikationen ihrer Bildgenerierungsmodelle gewinnen und sicherstellen, dass die generierten Bilder mit den Erwartungen und Werten der Benutzer übereinstimmen.

Durch eine aktive Zusammenarbeit mit externen Interessengruppen, Forschern und der breiteren Gemeinschaft können Organisationen wertvolle Erkenntnisse sammeln, Bedenken ansprechen und ihre Praktiken kontinuierlich verbessern. Dieser kollaborative Ansatz gewährleistet, dass die Entwicklung und Bereitstellung von KI-Bildgenerierungstechnologien mit gesellschaftlichen Werten und ethischen Standards im Einklang stehen.

\subsubsection{Partnerschaften über Sektorengrenzen hinweg}
Die Zusammenarbeit sollte über die Grenzen einer einzelnen Organisation oder Branche hinausgehen. Fördern Sie Partnerschaften mit anderen Institutionen, einschließlich akademischen Forschern, gemeinnützigen Organisationen, Regierungsbehörden und Branchenkollegen. Diese interdisziplinäre Zusammenarbeit ermöglicht eine breitere Perspektive und ein umfassenderes Verständnis der ethischen Implikationen der KI-Bildgenerierung. Durch die Nutzung des gemeinsamen Fachwissens und der Ressourcen verschiedener Sektoren können Organisationen komplexe Herausforderungen angehen und einen ganzheitlicheren Ansatz für ethische KI sicherstellen.

\subsubsection{Partizipative Gestaltung}
Beteiligung die Endbenutzer und Stakeholder am Design- und Entwicklungsprozess von KI-Bildgenerierungssystemen. Indem Organisationen diejenigen einbeziehen, die direkt von der Technologie betroffen sein werden, können sie wertvolle Einblicke gewinnen, spezifische Bedürfnisse identifizieren und Lösungen gemeinsam entwickeln, die den Benutzerwerten und ethischen Standards entsprechen. Partizipative Designmethoden wie nutzerzentriertes Design und Co-Creation-Workshops fördern die Zusammenarbeit, fördern Empathie und stellen sicher, dass KI-Systeme unter Berücksichtigung der Benutzerinteressen gestaltet werden.
\cite{Vogel}
\subsubsection{Öffentliche Beteiligung und Bewusstseinsbildung}
örderung Initiativen zur öffentlichen Beteiligung und Bewusstseinsbildung, um eine breitere Palette von Interessengruppen in den Dialog über die Ethik von KI-Bildgenerierung einzubeziehen. Führung öffentliche Konsultationen durch, organisieren Sie Workshops und Förderung öffentliche Debatten über die ethischen Implikationen von KI-Technologien. Diese Initiativen schaffen Raum für Diskussionen, fördern diverse Perspektiven und befähigen Einzelpersonen, zur Entwicklung ethischer Leitlinien und Richtlinien beizutragen.

Durch die Förderung der öffentlichen Beteiligung und Bewusstseinsbildung können Organisationen ein breiteres Spektrum an Perspektiven und Meinungen berücksichtigen. Die Einbeziehung der Öffentlichkeit kann helfen, Bedenken und Erwartungen bezüglich der Nutzung von KI-Technologien zu verstehen und somit auch Vertrauen aufzubauen.
\cite{Zytko}
\subsubsection{Kontinuierliche Überwachung und Evaluation}
Etablierung fortlaufender Überwachungs- und Evaluierungsprozesse zur Bewertung der ethischen Auswirkungen und Leistung von KI-Bildgenerierungssystemen. Regelmäßige Erfassung und Analyse von Rückmeldungen von Benutzern, betroffenen Gemeinschaften und anderen Interessengruppen, um potenzielle Voreingenommenheit, unbeabsichtigte Folgen oder aufkommende ethische Fragen zu identifizieren. Diese Feedbackschleife ermöglicht es Organisationen, Bedenken proaktiv anzugehen, ihre Modelle zu überarbeiten und sicherzustellen, dass ethische Praktiken während des gesamten Lebenszyklus von KI-Bildgenerierungssystemen eingehalten werden.
\cite{WILSON2022101652}
\subsubsection{Benutzerfeedback}
In dem Artikel \cite{Mittelstadt} argumentieren die Autoren, dass Benutzerfeedback und -beteiligung zu mehreren ethischen Dimensionen von KI-Algorithmen beitragen können, einschließlich Transparenz, Verantwortlichkeit und Fairness. Hier ist eine Erweiterung des Konzepts:

Transparenz: Benutzerfeedback kann dazu beitragen, die Transparenz von KI-Bildgenerierungssystemen zu erhöhen. Indem Benutzer in den Entwicklungsprozess einbezogen werden, können KI-Entwickler Informationen darüber teilen, wie das System funktioniert, seine Grenzen und die mögliche Voreingenommenheit, die es haben kann. Transparenz ermöglicht es den Benutzern, ein besseres Verständnis dafür zu haben, wie ihre Daten verwendet werden und wie das KI-System Bilder generiert.

Verantwortlichkeit: Benutzerfeedback spielt eine entscheidende Rolle bei der Rechenschaftspflicht von KI-Entwicklern und -Systemen. Es ermöglicht den Benutzern, Bedenken hinsichtlich potenzieller ethischer Probleme, Voreingenommenheit oder unbeabsichtigter Konsequenzen im Zusammenhang mit den generierten Bildern zu äußern. Entwickler können dieses Feedback nutzen, um Mängel zu identifizieren und zu beheben, die Algorithmen zu verfeinern und die Verantwortung für die Auswirkungen ihrer KI-Systeme zu übernehmen.

Fairness: Benutzerfeedback hilft dabei, potenzielle Voreingenommenheit im KI-Bildgenerierungsprozess zu identifizieren. Unterschiedliche Benutzer können unterschiedliche Erwartungen, kulturelle Kontexte oder Sensibilitäten haben. Indem sie mit einer vielfältigen Benutzergruppe interagieren, können Entwickler Voreingenommenheit erkennen und mindern, die zu unfairer oder diskriminierender Bildgenerierung führen könnten. Benutzerfeedback bietet die Möglichkeit sicherzustellen, dass das KI-System eine breite Palette von Perspektiven berücksichtigt und verschiedene kulturelle und gesellschaftliche Normen respektiert.

\section{Technische Methoden}

Technische Ansätze sind diese, die von den Entwicklern der IGAI sowohl während als auch nach der Entwicklungsphase eingesetzt werden können, beispielsweise in Form von Datenaufbereitung und -manipulation oder Filter. 
Aufgrund dessen werden in der weiteren Durchführung diese Ansätze danach gruppiert, ob diese in der Entwicklungsphase der IGAI oder in deren Anwendungsphase anzuwenden sind.

\subsection{Entwicklungsphase}
\subsubsection{Vermeidung von Datenverzerrungen im Trainingsdatensatz}


\paragraph{Kommentar Franke/Müller:}
Zweiter Absatz braucht nur eine Citation, kommt alles von der gleichen Quelle
Erster Satz soll mehr beschreiben, dass wenn der Wurm einmal drin ist, kriegt man ihn durch späteres ausbessern nicht mehr raus
dritter Absatz die überwachungsinstanz sollter weiter hervorgehben werden und nicht nur kurz am Ende rangeklatscht werden
Zweiter Absatz den Satzbau der Beispiele verbessern, die Sätze lesen sich schwer
vor die Aufzählung die jeweiligen Begriffe schreiben (in Reihenfolge): repräsentativ, quantitativ, qualitativ, Güte


Bereits bei der Zusammenstellung des Trainingsdatensatzes einer KI kann es zu Verzerrungen kommen, welche sich im späteren Verlauf der Entwicklung kaum mehr beheben lassen.
Diese beinhalten Stichprobenverzerrungen, Messwertverzerrungen, Kennzeichnungsverzerrungen, oder Verzerrung durch Einseitigkeit \cite{Srinivasan}.

Beispiele für Stichprobenverzerrungen können sein, dass zu wenig Daten in die Datensätze aufgenommen werden und dadurch nicht realitätsnah sind \cite{Srinivasan}.
Auch ist es möglich, dass die Daten systematisch einseitig sind. Ein Gesichtserkennungsmodell, welches hauptsächlich an hellhäutigen Gesichtern trainiert wurde,
kann so beispielweise bei dunkleren Hauttönen schlechter abschneiden, oder bestimmte Gesichtsformen und -farben gar nicht als solche wahrnehmen \cite{Srinivasan}.
Ebenfalls ist eine systematisch Messwertverzerrung möglich, etwa wenn Bilder, welche mit einer defekten Kamera aufgenommen worden, in die Trainingsdaten einbezogen werden.
Es kann auch vorkommen, dass Daten falsch oder unzuverlässig gekennzeichnet werden. So kann es vorkommen, dass eine Wiese als \textit{Rasen}, \textit{Gras}, \textit{Wiese} oder \textit{Weide} gekennzeichnet wird, damit existieren vier Bezeichnungen für das gleiche Objekt. Dies kann sich insgesamt negativ auf die Genauigkeit des Modells auswirken \cite{Srinivasan}.

Diese Verzerrung lassen sich vermeiden, indem man den Prozess der Datensammlung von neutraler Seite überwacht und verifiziert.
Bei dieser Überwachung sollten mindestens die genannten Punkte Beachtung finden: 
\begin{itemize}
    \item Die Daten müssen die Realität widerspiegeln und nicht nur Randgruppen oder Mehrheiten darstellen.
    \item Die Anzahl der Daten muss ausreichen um diese Realitätswiederspiegelung gewährleisten zu können.
    \item Die Kennzeichnung der Daten muss auf Voreingenommenheit und Redundanz geprüft werden.
    \item Die Daten müssen qualitativ hochwertig, mindestens aber für den Anwendungsfall ausreichend sein.
\end{itemize}

\subsubsection{Vermeidung von Verzerrungen in der Anforderungsformulierungen}
\paragraph{Kommentar Franke/Müller:}
Den Satz vom Beispiel drehen, alse mit der Kreditvergabe anfangen
Nochmal überlegen was jetzt nun Aufgabe und Anforderung sind und was die miteinander zu tun haben und das dann auch so im Text beibehalten, Vorschlag war auch Aufgabe ergibt sich Anforderung und diese Anforderung muss dann passen
zweiter Absatz eine Citation
dritter absatz neutraler Beobachter weiter nach vorne, wie im Kapitel davor auch


Bei den Definitionen einer Aufgabenstellung und den Anforderungen eines Modell kann es zu Verzerrungen kommen.

Diese Verzerrung kann einerseits durch die Formulierung der Aufgabenstellung entstehen, oder durch die Art der verwendeten Daten um das Ergebnis des Models zu erhalten \cite{Srinivasan}.
Ein Problem bei der Formulierung der Anforderungen an einen Algorithmus kann beispielsweise sein, Daten wie Geschlecht, Hautfarbe oder ethnische Zugehörigkeit in die 
Entscheidung einfließen zu lassen, ob einer Person ein Kredit gewährt wird. Hierdurch können sexistische oder rassistische Diskriminierungen bei der Entscheidung stattfinden \cite{Srinivasan}.

Bei der Formulierung der Anforderungen des Systems sollte eine Reihe an Aspekten berücksichtigt und durch einen neutralen Beobachter verifiziert werden:
\begin{itemize}
    \item Bei dem Treffen einer Entscheidung des Modells dürfen keine Aspekte berücksichtigt werden, die für ein erfolgreiches Ausführen des Algorithmus nicht nötig sind.
    \item Die Formulierung muss neutral gestaltet sein, das heißt durch die Formulierung an sich keine voreingenommenen Schlüsse ziehen lässt.
\end{itemize}

\subsubsection{Vermeidung von Verzerrungen bei der Modellbewertung}
Neben der Möglichkeit, dass die Trainingsdaten oder der Algorithmus Verzerrungen enthalten kann es auch sein, dass die Evaluation des Modells Verzerrungen aufweist.
Diese können durch menschliches Verhalten der Tester, oder durch schlecht entworfene Test-Frameworks auftreten \cite{Srinivasan}.

Ein menschliche Bewerter eines Modells können in veschiedenen Aspekten kritisch handeln. Sie können durch eigene Einstellungen und Erfahrungen auch unabsichtlich beeinflußt sein \cite{Srinivasan}.
Gerade wenn ein Bewerter direkt am Model beteiligt ist kann es zum so genannten Bestätigungsfehler, oder \glqq Peak-End-Effect\grqq{} kommen \cite{Srinivasan}.
Ein Bestätigungsfehler ist der Neigung durch subjektive Wahrnemung eine Erwartung bestätigt zu sehen \cite{Srinivasan}. 
Beim \glqq Peak-End-Effect\grqq{} wird eine Bewertung anhand von einzelnen Höhepunkten und dem letzten Eindruck geprägt \cite{Srinivasan}.
Ebenfalls kann ein entworfenes Test-Framework fehlerhaft sein, wenn darin zum Beispiel zu wenig getestet wird, oder mit nicht geeigneten Testdaten gearbeitet wird \cite{Srinivasan}.

Um ein Modell ausreichend zu testen sollten einige Punkte berücksichtigt werden:
\begin{itemize}
    \item Wenn Menschen das Modell bewerten sollten diese nicht direkt an der Entwicklung beteiligt sein, um so Neutralität zu wahren.
    \item Wenn das Modell durch Menschen evaluiert wird, sollte diese Evalutaion von unteschiedlichen Personen mit unterschiedlichen Hintergründen durchgeführt werden.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses mindestens eine für den Anwedungsfall annehmbare Menge an voneinander unabhängigen Tests durchführen.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses weitreichend auf systematische Fehler überprüft werden.
\end{itemize}

\subsubsection{Verwendung von Datenerweiterung (Data Augmentation)}
Data Augmentation beschreibt eine Technik Varianz in vorhandene Daten einzufügen und so die Datenmenge zu erweitern.
Im Fall von IGAIs könnten so Eingabetexte, mit welchen moralisch kritische Inhalte erzeugt werden könnten, im mehreren leicht oder stärker abgewandelten Formen der IGAI vorgegeben werden um die Konsistenz dieser zu überprüfen und besser anzutrainieren.  
Zusätzlich können Einschränkungen und Richtlinien in den Augmentationsprozess integriert werden, um sicherzustellen, dass bestimmte ethische Prinzipien eingehalten werden. 
Durch den Einsatz von Data Augmentation wird der Algorithmus besser darauf vorbereitet, moralische Grenzen zu respektieren und kontroverse oder anstößige Inhalte zu vermeiden. 
Die Verwendung von Data Augmentation ermöglicht somit eine Verbesserung der moralischen Ertragbarkeit des ML-Algorithmus bei der Generierung von Bildern aus Textinput.

Quelle: 
https://arxiv.org/pdf/1712.04621.pdf 
https://doi.org/10.48550/arXiv.1712.04621

\subsubsection{Verwendung von überwachtem Lernen (Supervised Learning)}
Die Trainingsdatensätze werden im Voraus bewertet und sortiert, bevor der Algorithmus durchlaufen wird. Nachdem der Algorithmus durchlaufen ist, wird das Ergebnis mit der Bewertung verglichen. 
Das Modell kann aus diesem Vergleich lernen, welches Verhalten oder Ergebnis richtig und welches falsch ist.

Nach diesem Prinzip könnte man für IGAIs Datensätze erstellen, welche aus 2-Tupeln bestehen: einem Text anhand dem ein Bild erzeugt werden soll und einem dazu passenden moralisch unbedenklichen Bild. 
Das Modell kann so anhand von positiven Beispielen lernen, welche Inhalte es erzeugen darf.

Weiter wäre auch das Gegenteil möglich: Ein geordnetes Paar an Texten und moralisch verwerflichen Bildern, anhand derer die KI lernen kann, was sie auf keinen Fall generieren darf.

\subsubsection{Verwendung von verstärktem Lernen (Reinforcement Learning)}
Wie in \ref{subsection:ethicsdefinition} erklärt, wird beim verstärkten Lernen ein Belohnungs-Bestrafungs-System eingesetzt. Dieses könnte im Fall von IGAIs daraus bestehen, einen zweiten Agenten zu verwenden, welcher die erzeugten Inhalte auf moralische Sauberkeit und dementsprechend den ersten Agenten, also die IGAI, bestraft oder belohnt.

\subsubsection{Verwendung von kontradiktorischen Training (adversarial training)}
Adversarial Learning ist eine Technik, die eingesetzt werden kann, um einen Machine Learning-Algorithmus moralisch ertragbarer zu gestalten. 
Dabei wird ein zusätzlicher Algorithmus, der als Adversary bezeichnet wird, verwendet, um den Hauptalgorithmus herauszufordern und zu verbessern. 

Der Adversary wird darauf trainiert, gezielt moralisch problematische Beispiele zu generieren, die der Hauptalgorithmus falsch klassifizieren könnte. 
Indem der Hauptalgorithmus mit solchen herausfordernden Beispielen konfrontiert wird, wird er dazu gezwungen, seine Entscheidungen sorgfältiger zu treffen und moralisch akzeptablere Ergebnisse zu erzielen. 

Durch diesen iterativen Prozess des Trainings und der Herausforderung kann der Algorithmus lernen, ethische Aspekte besser zu berücksichtigen und so seine Leistung im Hinblick auf Moral und Ethik zu verbessern. 

Adversarial Learning bietet somit eine vielversprechende Möglichkeit, ML-Algorithmen in Bezug auf moralische Aspekte weiterzuentwickeln und deren Einfluss auf die Gesellschaft verantwortungsvoll zu gestalten.

Quelle: https://arxiv.org/pdf/1611.01236.pdf

\subsection{Anwendungsphase}
Die Anwendungsphase ist der Zeitraum, nachdem eine IGAI fertig entwickelt wurde bis zu dem Zeitpunkt, an dem ihr Dienst eingestellt wird. In diesem Zeitraum wird die IGAI von den Nutzern verwendet. Techniken in der Anwendungsphase nehmen keinen direkten Einfluss auf die Funktionsweise der IGAI selbst. Die Techniken verändern die Rahmenbedingungen in denen die IGAI verwendet wird.
Michel will es überarbeiten und Einleitung machen bis 18.05 12:00:00
\subsubsection{Input-Filter}
Die Entwickler von IGAIs müssen davon ausgehen, dass Nutzer unethische Eingaben vornehmen. Damit ist der Input in Textform gemeint, den die IGAI verwendet, um daraus Bilder zu generieren. Um zu verhindern, dass diese Eingaben vorgenommen werden, können die Entwickler Input-Filter verwenden. Die Autoren (Artikel Artificial Intelligence as a Service for Immoral Content Detection and Eradication von Fadia Shah et al.) haben eine AI entwickelt, die unmoralischen Inhalt aus sozialen Netzwerken erkennen kann. Die Entwickler von IGAIs können anhand dieser Ergebnisse ihre eigene AI entwickeln, jedoch mit der Spezialisierung auf unethischen Inhalt anstatt unmoralischen Inhaltes. Diese AI können sie dann als Filter verwenden, um einzuschränken welche Eingaben überhaupt zur IGAI gelangen.
\subsubsection{Output-Filter}
Wenn eine IGAI Bilder generiert, müssen Entwickler davon ausgehen, dass dabei unethische Bilder generiert werden. Diese Bilder dürfen den Nutzer nicht erreichen. Um das zu verhindern können die Entwickler Output-Filter verwenden. Der Output ist das von der IGAI generierte Bild. Die Autoren (Blocking objectionable images: adult images and harmful symbols Huicheng Zheng et al.) haben ein System entwickelt, das aus zwei getrennten Filtern besteht: dem verbotene Symbole Filter und dem nicht jugendfreie Bilder Filter. Da diese Filter unabhängig voneinander arbeiten, kann das System um weitere Filter ergänzt werden. Die Entwickler haben damit die Möglichkeit alles zu filtern, was dem Nutzer nicht ausgegeben werden soll. Das System der Autoren war schneller und zuverlässig als seine Vorgänger. In Anbetracht des Alters des Artikels, ist anzunehmen, dass heutige Systeme noch schneller und zuverlässiger den zu filternden Inhalt ermitteln. Ein solches System kann von den Entwicklern verwendet werden, um den Output zu überwachen. Erkennt das System ein generiertes Bild als unethisch wird es nicht an den Nutzer ausgegeben. Außerdem können diese Ergebnisse auch zur Weiterentwicklung der IGAI selbst verwendet werden.
\subsubsection{Anwendungsbeschränkung}
Anwendungsbeschränkung bedeutet, dass Entwickler die Art und Weise wie Nutzer mit der IGAI interagieren einschränken. In diesem konkreten Fall soll es um eine Beschränkung der Anwendungsfrequenz gehen. Als Beispiel dient hierbei das Credit-System von DALL E2 von OpenAI. Bei diesem System werden Credits benötigt, um mittels der IGAI ein Bild zu generieren. Credits sind käuflich oder werden gestatten, wenn man sich in einem bestimmten Zeitraum nach Veröffentlichung von DALL E2 angemeldet hat. Wenn die IGAI ein Bild generiert, wird ein Credit verbraucht. Hat ein Nutzer keine Credits mehr, kann er keine Bilder mehr generieren. Dadurch wird eine initiale Barriere geschaffen, da Nutzer Credits kaufen müssen, um Bilder zu generieren. Nutzer, die auf die Generierung von unethischen Bildern abzielen, können damit abgeschreckt werden. Durch Anzahl der Credits kann zusätzlich beschränkt werden wie viele Bilder generiert werden können. Entwickler können dieses Credit-System nach ihrem Bedarf anpassen.

\chapter{Fazit}
\bibliographystyle{ieeetr}
\bibliography{Sources.bib}
\end{document}