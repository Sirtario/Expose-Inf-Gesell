\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Stand der Forschung \\[1ex] \large Thema: Welche Methodiken sind möglich bzw. werden angewandt, um die Erzeugung ethisch fragwürdiger Inhalte durch IGAI zu verhindern?}
\date{16.04.2023}
\author{Ronja Drechsler, \and Dominic Fitter, \and Michel Hecker, \and Khaldon Kassem, \and Phillip Eckstein}

\begin{document}

\maketitle
\tableofcontents
\section{Einleitung}
In den letzten Jahren haben Fortschritte in der Künstlichen Intelligenz (KI) dazu geführt, dass Bildgenerierende KI-Systeme immer 
leistungsfähiger geworden sind. Diese Systeme können nun hochauflösende Bilder in beispiellosem Detail generieren und haben das Potenzial,
 in vielen Branchen eingesetzt zu werden, einschließlich der Unterhaltungsindustrie, des Einzelhandels und des Gesundheitswesens. 
 Allerdings haben diese Fortschritte auch einige Bedenken hervorgerufen, insbesondere in Bezug auf die ethischen Implikationen, 
 die damit verbunden sind.
 in wichtiger Aspekt, der bei der Verwendung von bildgenerierenden KI-Systemen berücksichtigt werden muss, ist die Möglichkeit, 
 dass ethisch fragwürdige Inhalte generiert werden können. Es gibt ein wachsendes Bewusstsein für die Notwendigkeit, 
 sicherzustellen, dass KI-Systeme keine rassistischen, sexistischen oder anderweitig diskriminierenden Inhalte generieren. Diese 
 Herausforderung stellt eine wichtige Forschungsfrage dar, die untersucht werden muss, um sicherzustellen, dass die Verwendung von
  bildgenerierenden KI-Systemen ethisch vertretbar bleibt.

 \subsection{Motivation und Relevanz}
 Die Entwicklung von AI-Kunst reicht zurück bis in die 1960er Jahre\cite{Garcia}, aber erst in den letzten Jahren wurden 
 diese KIs durch die Veröffentlichungen von bahnbrechenden Projekten wie DALL-E (01/21), Midjourney (03/22) oder Stable Diffusion 
 (08/22) der Allgemeinheit zugänglich gemacht. Auslotungen der Leistungsfähigkeit dieser KIs zeigen jedoch auch auf, 
 dass sie sexistische oder rassistische Verzerrungen aufweisen \cite{Schmidt}und für moralisch fragwürdige Zwecke missbraucht werden 
 können, z. B. bei der Erstellung von Deepfakes, pornografischen oder gewalttätigen Inhalten.\cite{Hadero}

 Angesichts dessen haben u. a. die Entwickler von Midjourney reagiert und das Verwenden von Wörtern mit Bezug zur 
 Thematik menschlicher Fortpflanzungssysteme verboten \cite{Heikkilae} und ihre kostenlose Testversion eingestellt. 
 \cite{NelsonMidjourney} Doch es stellt sich die 
 Frage, wie Anbieter und Entwickler solcher KIs gegen diese schädliche Verwendung ihrer Technologie vorgehen können.
 
 Die vorliegende Arbeit wird sich mit dieser Frage auseinandersetzen und untersuchen, welche Möglichkeiten es gibt, um KIs vor 
 Missbrauch zu schützen. Sie wird diskutieren, wie die Anbieter und Entwickler dieser Technologie verantwortungsbewusst 
 handeln können, um sicherzustellen, dass ihre KIs nur für ethisch vertretbare Zwecke eingesetzt werden. Das Ziel ist es, 
 Wege aufzuzeigen, wie KI-Entwickler dazu beitragen können, die positiven Auswirkungen der Technologie zu maximieren, 
 während sie gleichzeitig die negativen Auswirkungen minimieren.
 \subsection{Forschungsfrage}
 In dieser Arbeit soll die Frage geklärt werden welche Methodiken möglich sind bzw. angewandt werden, um die Erzeugung ethisch 
 fragwürdiger Inhalte durch IGAI zu verhindern?
 \include{StandForschung.tex}
\include{Methodik.tex}
\section{Definitionen}
Wie in der Methodik erwähnt, werden im Folgenden die Begriffe der bildgenerienden KI und der Ethik im Bezug auf dieser für diese Arbeit definiert. 
Durch diese Vorgehensweise soll vermieden werden, dass diese Thematiken bei kommenden Durchführung immer neu angesprochen werden müssen und sich die Arbeit so auf klare und einheitliche Definitionen der Begriffe stützen kann.
\subsection{Ethik}\label{subsection:ethicsdefinition}
\subsection{AI}
Im Gegensatz zu bisherigen AIs können den generative artifical intelligence (GAI) neue Daten generieren, anstatt wie bisher Daten zu analysieren oder basierend auf gegebenen Daten zu handeln. Außerdem werden ihnen zum Trainieren enorme Datenmengen, 
wie der komplette Inhalt von Wikipedia, zur Verfügung gestellt. Daher sind diese AIs erst mit steigender Rechenkapazität möglich geworden. GAIs modellieren Netze aus miteinander verbundenen Daten in verschiedenen multimedialen Formaten. Demnach 
bestünden beispielsweise Verbindungen zwischen dem Wort Panda, ein Bild von einem Panda und einem Video von einem Panda. Dadurch können GAIs ein beliebiges Eingabeformat in ein beliebiges Ausgabeformat übersetzen. Thema dieser Arbeit sind image generating 
artifical intelligence (IGAI) oder auch text-to-image AIs, eine Unterkategorie von GAIs. Diese AIs können nach Eingabe von natürlichsprachlichem Text Bilder erzeugen.


Um GAIs zu trainieren, werden drei Arten von Machine Learning verwendet: überwachtes Lernen, unüberwachtes Lernen und verstärktes Lernen. 
Beim überwachten Lernen werden gekennzeichnete Datensätze verwendet, um Vorhersagen zu treffen. Die gekennzeichneten Daten werden als Trainingsdaten verwendet und nach dem Ausführen des Algorithmus wird überprüft, ob die getroffene Vorhersage der Kennzeichnung entspricht.
Beim unüberwachten Lernen wird dem Algorithmus nicht gekennzeichnete Daten zugeführt. Hierbei wird nicht versucht eine Vorhersage zu treffen, sondern Schlussfolgerungen oder Zusammenhänge zwischen den Daten zu finden.
Beim verstärkten Lernen wird ein System aus Belohnung und Strafen verwendet. Macht der Algorithmus wenige Fehler ist die Belohnung groß und die Strafe klein, macht der Algorithmus viele Fehler ist die Belohnung klein und die Strafe groß. Dadurch kann das Finden des 
optimalen Verhaltens automatisiert werden.

\section{Durchführung}
Verschiedene Formen der Voreignenommenheit von AI lassen sich durch unterschiedliche Methoden verhindern. Diese lassen sich allgemein in 
Technische und Nicht-Technische Methoden unterteilen. Nachfolgen werden verschiedene Methoden erläutert.
\subsection{Nicht-Technische Methoden}
Nicht-Technische Methoden sind solche, die nicht primär auf Technische Lösungen setzen.

Eine Möglichekeit des Vermeidens Von Voreignenommenheit ist der Einbezug von möglichst vieler Anteilhaber aus verschiedenen Gruppierungen. 
Ziel des Einbezugs ist es, möglichst viele unterschiedliche Perspektiven zu erhalten, was dabei helfen soll mögliche Voreignenommenheiten zu erkennen und vermeiden.


\subsection{Technische Methoden}
Technische Ansätze sind diese, die von den Entwicklern der IGAI sowohl während als auch nach der Entwicklungsphase eingesetzt werden können, beispielsweise in Form von Datenaufbereitung und -manipulation oder Filter. 
Aufgrund dessen werden in der weiteren Durchführung diese Ansätze danach gruppiert, ob diese in der Entwicklungsphase der IGAI oder in deren Anwendungsphase anzuwenden sind.

\subsubsection{Vermeidung von Datenverzerrungen im Trainingsdatensatz}
Bereits bei der Zusammenstellung des Trainingsdatensatzes einer KI kann es zu Verzerrungen kommen, welche sich im späteren Verlauf der Entwicklung kaum mehr beheben lassen.
Diese beinhalten Stichprobenverzerrungen, Messwertverzerrungen, Kennzeichnungsverzerrungen, oder Verzerrung durch Einseitigkeit \cite{Srinivasan}.

Beispiele für Stichprobenverzerrungen können sein, dass zu wenig Daten in die Datensätze aufgenommen werden und dadurch nicht realitätsnah sind \cite{Srinivasan}.
Auch ist es möglich, dass die Daten systematisch einseitig sind. Ein Gesichtserkennungsmodell, welches hauptsächlich an hellhäutigen Gesichtern trainiert wurde,
kann so beispielweise bei dunkleren Hauttönen schlechter abschneiden, oder bestimmte Gesichtsformen und -farben gar nicht als solche wahrnehmen \cite{Srinivasan}.
Ebenfalls ist eine systematisch Messwertverzerrung möglich, etwa wenn Bilder, welche mit einer defekten Kamera aufgenommen worden, in die Trainingsdaten einbezogen werden.
Es kann auch vorkommen, dass Daten falsch oder unzuverlässig gekennzeichnet werden. so kann es vorkommen. Eine Wiese kann als \textit{Rasen}, \textit{Gras}, \textit{Wiese} oder \textit{Weide} gekennzeichnet werden und erhält damit vier Bezeichnungen für das gleiche Objekt existieren. Dies kann sich insgesamt negativ auf die Genauigkeit des Modells auswirken \cite{Srinivasan}.

Diese Verzerrung lassen sich vermeiden, indem man den Prozess der Datensammlung von neutraler Seite überwacht und verifiziert.
Bei dieser Überwachung sollten mindestens die genannten Punkte Beachtung finden: 
\begin{itemize}
    \item Die Daten müssen die Realität widerspiegeln und nicht nur Randgruppen oder Mehrheiten darstellen.
    \item Die Anzahl der Daten muss ausreichen um diese Realitätswiederspiegelung gewährleisten zu können.
    \item Die Kennzeichnung der Daten muss auf Voreingenommenheit und Redundanz geprüft werden.
    \item Die Daten müssen qualitativ hochwertig, mindestens aber für den Anwendungsfall ausreichend sein.
\end{itemize}

\subsubsection{Vermeidung von Verzerrungen in der Anforderungsformulierungen}
Bei den Definitionen einer Aufgabenstellung und den Anforderungen eines Modell kann es zu Verzerrungen kommen.

Diese Verzerrung kann einerseits durch die Formulierung der Aufgabenstellung entstehen, oder durch die Art der verwendeten Daten um das Ergebnis des Models zu erhalten \cite{Srinivasan}.
Ein Problem bei der Formulierung der Anforderungen an einen Algorithmus kann beispielsweise sein, Daten wie Geschlecht, Hautfarbe oder ethnische Zugehörigkeit in die 
Entscheidung einfließen zu lassen, ob einer Person ein Kredit gewährt wird. Hierdurch können sexistische oder rassistische Diskriminierungen bei der Entscheidung stattfinden \cite{Srinivasan}.

Bei der Formulierung der Anforderungen des Systems sollte eine Reihe an Aspekten berücksichtigt und durch einen neutralen Beobachter verifiziert werden:
\begin{itemize}
    \item Bei dem Treffen einer Entscheidung des Modells dürfen keine Aspekte berücksichtigt werden, die für ein erfolgreiches Ausführen des Algorithmus nicht nötig sind.
    \item Die Formulierung muss neutral gestaltet sein, das heißt durch die Formulierung an sich keine voreingenommenen Schlüsse ziehen lässt.
\end{itemize}

\subsubsection{Vermeidung von Verzerrungen bei der Modellbewertung}
Neben der Möglichkeit, dass die Trainingsdaten oder der Algorithmus Verzerrungen enthalten kann es auch sein, dass die Evaluation des Modells Verzerrungen aufweist.
Diese können durch menschliches Verhalten der Tester, oder durch schlecht entworfene Test-Frameworks auftreten \cite{Srinivasan}.

Ein menschliche Bewerter eines Modells können in veschiedenen Aspekten kritisch handeln. Sie können durch eigene Einstellungen und Erfahrungen auch unabsichtlich beeinflußt sein \cite{Srinivasan}.
Gerade wenn ein Bewerter direkt am Model beteiligt ist kann es zum so genannten Bestätigungsfehler, oder \glqq Peak-End-Effect\grqq{} kommen \cite{Srinivasan}.
Ein Bestätigungsfehler ist der Neigung durch subjektive Wahrnemung eine Erwartung bestätigt zu sehen \cite{Srinivasan}. 
Beim \glqq Peak-End-Effect\grqq{} wird eine Bewertung anhand von einzelnen Höhepunkten und dem letzten Eindruck geprägt \cite{Srinivasan}.
Ebenfalls kann ein entworfenes Test-Framework fehlerhaft sein, wenn darin zum Beispiel zu wenig getestet wird, oder mit nicht geeigneten Testdaten gearbeitet wird \cite{Srinivasan}.

Um ein Modell ausreichend zu testen sollten einige Punkte berücksichtigt werden:
\begin{itemize}
    \item Wenn Menschen das Modell bewerten sollten diese nicht direkt an der Entwicklung beteiligt sein, um so Neutralität zu wahren.
    \item Wenn das Modell durch Menschen evaluiert wird, sollte diese Evalutaion von unteschiedlichen Personen mit unterschiedlichen Hintergründen durchgeführt werden.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses mindestens eine für den Anwedungsfall annehmbare Menge an voneinander unabhängigen Tests durchführen.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses weitreichend auf systematische Fehler überprüft werden.
\end{itemize}

\subsubsection{Verwendung von Datenerweiterung (Data Augmentation)}

\subsubsection{Verwendung von überwachtem Lernen (Supervised Learning)}
Die Trainingsdatensätze werden im Voraus bewertet und sortiert, bevor der Algorithmus durchlaufen wird. Nachdem der Algorithmus durchlaufen ist, wird das Ergebnis mit der Bewertung verglichen. 
Das Modell kann aus diesem Vergleich lernen, welches Verhalten oder Ergebnis richtig und welches falsch ist.

Nach diesem Prinzip könnte man für IGAIs Datensätze erstellen, welche aus 2-Tupeln bestehen: einem Text anhand dem ein Bild erzeugt werden soll und einem dazu passenden moralisch unbedenklichen Bild. 
Das Modell kann so anhand von positiven Beispielen lernen, welche Inhalte es erzeugen darf.

Weiter wäre auch das Gegenteil möglich: Ein geordnetes Paar an Texten und moralisch verwerflichen Bildern, anhand derer die KI lernen kann, was sie auf keinen Fall generieren darf.

\subsubsection{Verwendung von verstärktem Lernen (Reinforcement Learning)}
Wie in \ref{subsection:ethicsdefinition} erklärt, wird beim verstärkten Lernen ein Belohnungs-Bestrafungs-System eingesetzt. Dieses könnte im Fall von IGAIs daraus bestehen, einen zweiten Agenten zu verwenden, welcher die erzeugten Inhalte auf moralische Sauberkeit und dementsprechend den ersten Agenten, also die IGAI, bestraft oder belohnt.

\subsubsection{Verwendung von kontradiktorischen Training (adversarial training)}



\section{Fazit}
\bibliographystyle{ieeetr}
\bibliography{Sources.bib}
\end{document}