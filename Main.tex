\documentclass[12pt]{report}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}

\title{Erster Forschungsbericht \\[1ex] \large Thema: Welche Methodiken sind möglich bzw. werden angewandt, um die Erzeugung ethisch fragwürdiger Inhalte durch IGAI zu verhindern?}
\date{21.05.2023}
\author{Ronja Drechsler, \and Dominic Fitter, \and Michel Hecker, \and Khaldon Kassem, \and Phillip Eckstein}

\begin{document}

\maketitle
\tableofcontents
\newpage

\chapter{Einleitung}

Die Entwicklung von Kunst mithilfe von KI reicht zurück bis in die 1960er Jahre\cite{Garcia}, aber erst in den letzten Jahren wurden diese KIs durch die Veröffentlichungen von Projekten wie DALL-E (01/21), Midjourney (03/22) oder Stable Diffusion (08/22) der Allgemeinheit zugänglich gemacht. Auslotungen der Leistungsfähigkeit dieser KIs zeigen jedoch auf, dass sie sexistische oder rassistische Verzerrungen aufweisen \cite{Schmidt} und für moralisch fragwürdige Zwecke missbraucht werden können, z. B. zur Erstellung von Deepfakes, pornografischen oder gewalttätigen Inhalten \cite{Hadero}.
Angesichts dessen haben u. a. die Entwickler von Midjourney reagiert und das Verwenden von Wörtern mit Bezug zur Thematik menschlicher Fortpflanzungssysteme verboten \cite{Heikkilae} und ihre kostenlose Testversion eingestellt \cite{NelsonMidjourney}. Daraus ergibt sich die Frage, welche Methodiken möglich sind bzw. angewandt werden, um die Erzeugung ethisch fragwürdiger Inhalte durch IGAI zu verhindern. 
Die vorliegende Arbeit setzt sich mit dieser Forschungsfrage auseinander, indem sie untersucht, welche Methoden angewandt werden, um KIs vor Missbrauch zu schützen. Sie diskutiert, wie die Anbieter und Entwickler dieser Technologie verantwortungsbewusst handeln können, um sicherzustellen, dass ihre KIs nur für ethisch vertretbare Zwecke eingesetzt werden. Das Ziel besteht darin, aufzuzeigen wie IGAI-Entwickler dazu beitragen können, dass mit ihrer Technologie keine unethischen Inhalte erzeugt werden.

\include{StandForschung.tex}
\include{Methodik.tex}
\chapter{Definitionen}
Wie in Kapitel Methodik erwähnt, werden im Folgenden die Begriffe der bildgenerienden KI und der Ethik im Bezug auf dieser für diese Arbeit definiert.
\section{Ethik}

%was Herr Müller sich gewünscht hatte:
%das wird ja von unterschiedlichen Ebenen beschlossen (un gibt Resolution, EU setzt das in Richtlinien um, de muss das in nationales Recht übertragen) wir haben uns auf EU eben geeinigt, er möchte den Prozess mal kurz haben, warum wir uns darauf festgelegt haben (ich nehme da dann an, weil das noch nicht in nationales recht übernommen wurde?) und dann halt noch mal was da dann explizit drin steht (das müsste von khaldon schon da sein, da hätte sich Herr Müller aber nen eigenen text gewünscht statt den text zu Ctrl c + V)


\glqq Die Welt benötigt Regeln für künstliche Intelligenz, um der Menschheit zugute zu kommen. Die Empfehlung zur Ethik der KI ist eine wichtige Antwort darauf. Sie legt den ersten globalen normativen Rahmen fest und gibt den Staaten die Verantwortung, ihn auf ihrer Ebene anzuwenden. Die UNESCO wird ihre 193 Mitgliedsstaaten bei der Umsetzung unterstützen und sie bitten, regelmäßig über ihre Fortschritte und Praktiken zu berichten.\glqq{} \\\textbf{--- Audrey Azoulay, Generaldirektorin der UNESCO}
\paragraph{UNESCO\cite{UNESCO}\\}
Im November 2019 verabschiedete die Generalversammlung der Organisation der Vereinten Nationen für Bildung, Wissenschaft und Kultur (\textbf{UNESCO}) auf ihrer 40. Sitzung die 40 C/Resolution 37, durch die sie die Generaldirektorin beauftragte, \glqq ein international gültiges Instrument zur Festlegung von Standards in Bezug auf die Ethik der künstlichen Intelligenz (KI) in Form einer Empfehlung vorzubereiten\glqq{}.\\Im November 2021 wurde auf der 41. Sitzung der \textbf{UNESCO} diese Empfehlung von den Mitgliedstaaten als weltweit erste globale Norm für die Ethik der künstlichen Intelligenz angenommen.\\

Diese Empfehlung basiert auf einer Reihe von Werten und Grundsätzen, die die Mitgliedstaaten ermutigen, sie freiwillig bei der Nutzung künstlicher Intelligenz einzuhalten, um die Achtung der Menschenrechte, der Grundfreiheiten und der Rechtsstaatlichkeit in der digitalen Welt ebenso wie in der realen Welt sicherzustellen und im Einklang mit den nationalen Gesetzen in jedem Land zu handeln.

\paragraph{Werte:}
\begin{itemize}
	\item Respect, protection and promotion of human rights and fundamental freedoms and human dignity
	\item Environment and ecosystem flourishing
	\item Ensuring diversity and inclusiveness
	\item Living in peaceful, just and interconnected societies
\end{itemize}

\paragraph{Grundsätze:}
\begin{itemize}
	\item  Proportionality and Do No Harm
	\item  Safety and security
	\item  Fairness and non-discrimination
	\item  Sustainability
	\item  Right to Privacy, and Data Protection
	\item  Human oversight and determination
	\item  Transparency and explainability
	\item  Responsibility and accountability
	\item  Awareness and literacy
	\item  Multi-stakeholder and adaptive governance and collaboration
\end{itemize}
\paragraph{EU\cite{EUCommision}\\}
Im April 2019 veröffentlichte die Kommission der Europäischen Union (\textbf{EU}) \glqq ETHIK-LEITLINIEN FÜR EINE VERTRAUENSWÜRDIGE KI\glqq{}.\\Während des gesamten Lebenszyklus eines Systems muss KI drei Komponenten erfüllen, um vertrauenswürdig zu sein: a) rechtmäßig, b) ethisch und c) robust. Diese Komponenten müssen eine Reihe von Grundsätzen und Werten, die bei der Entwicklung, Einführung und Verwendung von KI-Systemen gewahrt werden.

\paragraph{Werte:}
\begin{itemize}
	\item  Verhältnismäßigkeit und Schadensvermeidung
	\item  Sicherheit und Schutz
	\item  Fairness und Nichtdiskriminierung
	\item  Nachhaltigkeit
	\item  Recht auf Privatsphäre und Datenschutz
	\item  Menschliche Aufsicht und Entscheidungsbefugnis
	\item  Transparenz und Erklärbarkeit
	\item  Verantwortung und Rechenschaftspflicht
	\item  Bewusstsein und Bildung
	\item Multi-Stakeholder und adaptive Governance und Zusammenarbeit
\end{itemize}

\paragraph{Grundsätze:}
\begin{itemize}
	\item  Achtung der menschlichen Autonomie
	\item  Schadensverhütung
	\item  Fairness
	\item  Erklärbarkeit
\end{itemize}

Aufbauend auf diesen Grundsätzen wird  Vertrauenswürdige KI mithilfe eine Liste von sieben Anforderungen umgesetzt und verwirklicht, die erfüllt werden müssen, und durch sowohl technische als auch nicht-technische Ansätze während des gesamten Lebenszyklus eines KI-Systems umgesetzt.

\paragraph{Anforderungen an eine vertrauenswürdige KI:}
\begin{enumerate}
	\item  Vorrang menschlichen Handelns und menschliche Aufsicht
	\item  Technische Robustheit und Sicherheit
	\item  Schutz der Privatsphäre und Datenqualitätsmanagement
	\item  Transparenz
	\item  Vielfalt, Nichtdiskriminierung und Fairness
	\item  Gesellschaftliches und ökologisches Wohlergehen
	\item  Rechenschaftspflicht
\end{enumerate}

Seit April 2021 liegt ein Vorschlag der Europäischen Kommission vor, einheitliche Regeln für künstliche Intelligenz festzulegen (Gesetz über künstliche Intelligenz), der einige Rechtssysteme in der Union ändern soll (COM(2021) 206 final).

Vor dem Hintergrund dieses Vorschlags haben die Regierungen einiger Länder, wie beispielsweise der Bundesrepublik Deutschland, versucht zu prüfen, ob in Bezug auf die diesbezüglichen Empfehlungen der UNESCO weiterer Handlungsbedarf besteht, während die Studie des Deutschen Komitees für die UNESCO in (UNESCO-Empfehlung am Die Ethik der Künstlichen Intelligenz – Bedingungen für die Umsetzung in Deutschland) wird berücksichtigt.\cite{Deutscher_Bundestag}



\section{AI}\label{def_ai}
Generative artifical intelligence (GAI) können neue Daten generieren, im Gegensatz zu vor deren Einführung entwickelten AIs, die nur Daten analysieren oder basierend auf gegebenen Daten handeln. Außerdem werden ihnen zum Trainieren enorme Datenmengen, 
wie der komplette Inhalt von Wikipedia, zur Verfügung gestellt. Daher sind diese AIs erst mit steigender Rechenkapazität möglich geworden. GAIs modellieren Netze aus miteinander verbundenen Daten in verschiedenen multimedialen Formaten. Demnach 
bestünden beispielsweise Verbindungen zwischen dem Wort Panda, ein Bild von einem Panda und einem Video von einem Panda. Dadurch können GAIs ein beliebiges Eingabeformat in ein beliebiges Ausgabeformat übersetzen. Thema dieser Arbeit sind image generating 
artifical intelligence (IGAI) oder auch text-to-image AIs, eine Unterkategorie von GAIs. Diese AIs können nach Eingabe von natürlichsprachlichem Text Bilder erzeugen.
\cite{Roberto}

Um GAIs zu trainieren, werden drei Arten von Machine Learning verwendet: überwachtes Lernen, unüberwachtes Lernen und verstärktes Lernen. 
Beim überwachten Lernen werden gekennzeichnete Datensätze verwendet, um Vorhersagen zu treffen. Die gekennzeichneten Daten werden als Trainingsdaten verwendet und nach dem Ausführen des Algorithmus wird überprüft, ob die getroffene Vorhersage der Kennzeichnung entspricht.
Beim unüberwachten Lernen wird dem Algorithmus nicht gekennzeichnete Daten zugeführt. Hierbei wird nicht versucht eine Vorhersage zu treffen, sondern Schlussfolgerungen oder Zusammenhänge zwischen den Daten zu finden.
Beim verstärkten Lernen wird ein System aus Belohnung und Strafen verwendet. Macht der Algorithmus wenige Fehler ist die Belohnung groß und die Strafe klein, macht der Algorithmus viele Fehler ist die Belohnung klein und die Strafe groß. Dadurch kann das Finden des 
optimalen Verhaltens automatisiert werden.
\cite{serafeim}

\chapter{Durchführung}
Verschiedene Formen der Voreingenommenheit von AI lassen sich durch unterschiedliche Methoden verhindern. Nachfolgend werden die Methoden erläutert, wobei sie in technische und nicht-technische unterteilt werden.

\section{Nicht-Technische Methoden}
nicht-technische Methoden sind solche, die nicht primär auf technische Lösungen setzen.\\
Unter Berücksichtigung der UNESCO-Empfehlung\cite{UNESCO} und der Leitlinien der EU-Kommission\cite{EUCommision} zur KI-Ethik können einige nichttechnische Methoden identifiziert werden, die angewendet werden können.

\subsection{Datenverzerrungen im Trainingsdatensatz}
Bereits bei der Zusammenstellung des Trainingsdatensatzes einer KI kann es zu Verzerrungen kommen. Würde eine KI durch einen so verzerrten Datensatz trainiert werden, könnte sich in ihr diese Verzerrung wiederspiegeln. Eine so antrainierte Verzerrung lässt sich, wenn überhaupt, nur sehr aufwendig wieder korrigieren, es empfiehlt sich also von vorherein einen geeigneten Datensatz zu verwenden.
Diese Verzerrungen können dabei Stichprobenverzerrungen, Messwertverzerrungen, Kennzeichnungsverzerrungen, oder Verzerrung durch Einseitigkeit sein \cite[S. 48ff.]{Srinivasan}.

Beispiele für Stichprobenverzerrungen können sein, dass zu wenig Daten in die Datensätze aufgenommen werden und diese keine realitätsnahe Darstellung ermöglichen.
Auch ist es möglich, dass die Daten systematisch einseitig sind. Ein Gesichtserkennungsmodell, das hauptsächlich an hellhäutigen Gesichtern trainiert wurde, kann beispielweise bei dunkleren Hauttönen schlechter abschneiden oder bestimmte Gesichtsformen und -farben gar nicht als Gesichter erkennen.
Zudem ist eine systematisch Messwertverzerrung möglich, etwa wenn Bilder, die mit einer nicht vollständig funktionstüchtigen Kamera aufgenommen worden, in die Trainingsdaten einbezogen werden.
Es kann auch vorkommen, dass Daten falsch, überflüssig oder unzuverlässig gekennzeichnet werden. Bespielsweise ist die Unterscheidung zwischen einer \textit{Wiese} und \textit{Gras} selten nötig. Existieren aber beide Label im Trainingsdatensatz einer KI kann sich dies negativ auf die Genauigkeit des Modells auswirken, wenn bei der Beschriftung nicht konsequent beide Begriffe unterschieden werden\cite[S. 48ff.]{Srinivasan}. 

Eine Überwachungsinstanz in Form eines neutralen Beobachter oder Systems zuzüglich einer reflektierten Selbstkontrolle der Datensammler kann dabei helfen solche Verzerrungen zu vermeiden.
Bei dieser Überwachung sollten mindestens die genannten Punkte Beachtung finden:
\begin{itemize}

    \item Repräsentativität: Die Daten müssen die Realität widerspiegeln und nicht nur Randgruppen oder Mehrheiten darstellen.
    \item Quantität: Die Anzahl der Daten muss ausreichen,  um eine Wiederspiegelung der Realität gewährleisten zu können.
    \item Qualität: Die Kennzeichnung der Daten muss auf Voreingenommenheit und Redundanz geprüft werden.
    \item Güte: Die Daten müssen qualitativ hochwertig, mindestens aber für den Anwendungsfall ausreichend sein.

\end{itemize}

\subsection{Verzerrungen in der Anforderungsformulierungen}
Bei den Definitionen einer Aufgabenstellung und der Anforderungen an ein Modell kann es zu Verzerrungen kommen.

Diese Verzerrung kann einerseits durch die Formulierung der Aufgabenstellung aus den Anforderungen oder durch die Art der verwendeten Daten entstehen.  \cite[S. 51f.]{Srinivasan}

Ein Problem bei der Formulierung der Anforderungen an einen Algorithmus kann beispielsweise darin bestehen, wenn Daten wie Geschlecht, Hautfarbe oder ethnische Zugehörigkeit in die Entscheidung einfließen, ob einer Person ein Kredit gewährt wird. Hierdurch können sexistische oder rassistische Diskriminierungen bei der Entscheidung stattfinden \cite[S. 51f.]{Srinivasan}.

Es empfiehlt sich das Einsetzten eines neutralen Beobachters, welcher die Anforderungen und daraus enstehende Aufgabenstellung auf folgende Aspekte prüf
\begin{itemize}
    \item Bei dem Treffen einer Entscheidung des Modells dürfen keine Aspekte berücksichtigt werden, die für eine erfolgreiche Ergebnisfindung der Aufgabenstellung nicht nötig sind.
    \item Die Formulierung muss neutral gestaltet sein, sodass sich durch die Formulierung an sich keine Schlüsse ziehen lassen, die mit Voreingenommenheit einhergehen.
\end{itemize}

\subsection{Vermeidung von Verzerrungen bei der Modellbewertung}
Neben der Möglichkeit, dass die Trainingsdaten oder der Algorithmus Verzerrungen enthalten, kann es auch sein, dass die Evaluation des Modells Verzerrungen aufweist.
Diese können durch menschliches Verhalten der Tester, oder durch schlecht entworfene Test-Frameworks auftreten \cite[S. 54f.]{Srinivasan}.

Menschliche Bewerter eines Modells können auf verschiedene Arten unethisch handeln. Sie können durch eigene Einstellungen und Erfahrungen beeinflusst sein, auch unbeabsichtigt.
Gerade wenn ein Bewerter direkt am Model beteiligt ist kann es zum so genannten Bestätigungsfehler, oder \glqq Peak-End-Effect\grqq{} kommen. Ein Bestätigungsfehler ergibt sich aus der Neigung, durch subjektive Wahrnehmung eine Erwartung bestätigt zu sehen. Ist so beispielsweise der Beurteiler beruflich oder finanziell direkt vom Erfolg des Projekts abhängig ist es eher möglich, dass Fehler nicht auffallen oder relativiert werden, als wenn er es nicht wäre.
Beim \glqq Peak-End-Effect\grqq{} wird eine Bewertung anhand von einzelnen Besonderheiten oder dem letzten Eindruck geprägt.
Ferner kann ein entworfenes Test-Framework fehlerhaft sein, wenn darin zum Beispiel zu wenig getestet wird, oder mit nicht geeigneten Testdaten gearbeitet wird \cite[S. 54f.]{Srinivasan}.

Um ein Modell ausreichend zu testen sollten einige Punkte berücksichtigt werden:
\begin{itemize}
    \item Wenn Menschen das Modell bewerten, sollten diese nicht direkt an der Entwicklung beteiligt sein, um Neutralität zu gewährleisten.
    \item Wenn das Modell durch Menschen evaluiert wird, sollte diese Evalutaion von mehreren Personen mit unterschiedlichen Hintergründen durchgeführt werden.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses mindestens eine für den Anwendungsfall annehmbare Menge an voneinander unabhängigen Tests durchführen.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses weitreichend auf systematische Fehler überprüft werden.
\end{itemize}


\subsection{Abgrenzung des rechtlichen Rahmens durch AGBs und TOS}
Eine Etablierung von allgemeinen Geschäftsbedingungen (AGBs) oder auch Terms of Service (TOS) legt den rechtlichen Rahmen für die Nutzung von IGAIs fest. Dadurch können sie dazu beitragen, den Missbrauch solcher Systeme einzudämmen. 
Allerdings sind AGBs allein nicht hinreichend, um den Missbrauch von IGAIs gänzlich zu verhindern, da sie keine aktive Nutzungsbeschränkung bieten. Wesentliche Aspekte könnten sein:
\begin{itemize}
    \item Festlegung des Nutzungszwecks: AGBs oder TOS sollten explizit den beabsichtigten Nutzungszweck der KI definieren. Dadurch werden ein Rahmen für akzeptable Aktivitäten abgesteckt und unerwünschte Nutzungsformen ausgeschlossen.
    \item Untersagung unzulässiger Aktivitäten: AGBs sollten eine Liste von Aktivitäten enthalten, die als Missbrauch der KI gelten und somit untersagt sind.
    \item Haftungsausschluss: Ein Haftungsausschluss in den AGBs ist bedeutsam, um zu verdeutlichen, dass der Anbieter der KI nicht für den Missbrauch der Technologie durch die Nutzer verantwortlich gemacht werden kann. Dadurch sollen potenzielle rechtliche Konsequenzen für den Anbieter eingeschränkt werden.
    \item Festlegung von Nutzungsregeln: Die AGBs  können spezifische Regeln für die Nutzer der KI enthalten, um den Missbrauch einzudämmen. Dies könnte beispielsweise die Verpflichtung zur Registrierung, die Verwendung sicherer Zugangsdaten oder die Einhaltung ethischer Richtlinien umfassen.
    \item Überwachung und Sanktionen: AGBs sollten auch klare Informationen darüber bereitstellen, wie die Nutzung der KI überwacht wird und welche Sanktionen im Falle von Missbrauch verhängt werden können. Dies kann beispielsweise die Möglichkeit zur Überprüfung von Nutzeraktivitäten, die Sperrung von Konten oder die Meldung rechtswidrigen Verhaltens umfassen.
\end{itemize}

\subsection{Zusammenarbeit und Feedback}
Benutzerfeedback und Engagement beinhalten das aktive Einbeziehen von Benutzern während des Entwicklungsprozesses und die Integration ihrer Perspektiven, Bedenken und Werte in die Gestaltung und Entscheidungsfindung von KI-Systemen. Durch die Berücksichtigung des Benutzerfeedbacks können KI-Entwickler wertvolle Einblicke in die potenziellen ethischen Implikationen ihrer Bildgenerierungsmodelle gewinnen und sicherstellen, dass die generierten Bilder mit den Erwartungen und Werten der Benutzer übereinstimmen.

Durch eine aktive Zusammenarbeit mit externen Interessengruppen, Forschern und der breiteren Gemeinschaft können Organisationen wertvolle Erkenntnisse sammeln, Bedenken ansprechen und ihre Praktiken kontinuierlich verbessern. Dieser kollaborative Ansatz gewährleistet, dass die Entwicklung und Bereitstellung von KI-Bildgenerierungstechnologien mit gesellschaftlichen Werten und ethischen Standards im Einklang stehen.\cite{EUCommision}\cite{UNESCO}

\subsubsection{Partnerschaften über Sektorengrenzen hinweg}
Die Zusammenarbeit sollte über die Grenzen einer einzelnen Organisation oder Branche hinausgehen. Fördern Sie Partnerschaften mit anderen Institutionen, einschließlich akademischen Forschern, gemeinnützigen Organisationen, Regierungsbehörden und Branchenkollegen. Diese interdisziplinäre Zusammenarbeit ermöglicht eine breitere Perspektive und ein umfassenderes Verständnis der ethischen Implikationen der KI-Bildgenerierung. Durch die Nutzung des gemeinsamen Fachwissens und der Ressourcen verschiedener Sektoren können Organisationen komplexe Herausforderungen angehen und einen ganzheitlicheren Ansatz für ethische KI sicherstellen.
\cite{Vogel}
\subsubsection{Partizipative Gestaltung}
Beteiligung die Endbenutzer und Stakeholder am Design- und Entwicklungsprozess von KI-Bildgenerierungssystemen. Indem Organisationen diejenigen einbeziehen, die direkt von der Technologie betroffen sein werden, können sie wertvolle Einblicke gewinnen, spezifische Bedürfnisse identifizieren und Lösungen gemeinsam entwickeln, die den Benutzerwerten und ethischen Standards entsprechen. Partizipative Designmethoden wie nutzerzentriertes Design und Co-Creation-Workshops fördern die Zusammenarbeit, fördern Empathie und stellen sicher, dass KI-Systeme unter Berücksichtigung der Benutzerinteressen gestaltet werden.
\cite{Zytko}
\subsubsection{Öffentliche Beteiligung und Bewusstseinsbildung}
Förderung Initiativen zur öffentlichen Beteiligung und Bewusstseinsbildung, um eine breitere Palette von Interessengruppen in den Dialog über die Ethik von KI-Bildgenerierung einzubeziehen. Führung öffentliche Konsultationen durch, organisieren Sie Workshops und Förderung öffentliche Debatten über die ethischen Implikationen von KI-Technologien. Diese Initiativen schaffen Raum für Diskussionen, fördern diverse Perspektiven und befähigen Einzelpersonen, zur Entwicklung ethischer Leitlinien und Richtlinien beizutragen.
\cite{WILSON2022101652}
\subsubsection{Kontinuierliche Überwachung und Evaluation}
Etablierung fortlaufender Überwachungs- und Evaluierungsprozesse zur Bewertung der ethischen Auswirkungen und Leistung von KI-Bildgenerierungssystemen. Regelmäßige Erfassung und Analyse von Rückmeldungen von Benutzern, betroffenen Gemeinschaften und anderen Interessengruppen, um potenzielle Voreingenommenheit, unbeabsichtigte Folgen oder aufkommende ethische Fragen zu identifizieren. Diese Feedbackschleife ermöglicht es Organisationen, Bedenken proaktiv anzugehen, ihre Modelle zu überarbeiten und sicherzustellen, dass ethische Praktiken während des gesamten Lebenszyklus von KI-Bildgenerierungssystemen eingehalten werden.
\cite{EUCommision}
\subsubsection{Benutzerfeedback}
In dem Artikel \cite{Mittelstadt} argumentieren die Autoren, dass Benutzerfeedback und -beteiligung zu mehreren ethischen Dimensionen von KI-Algorithmen beitragen können, einschließlich Transparenz, Verantwortlichkeit und Fairness. Hier ist eine Erweiterung des Konzepts:

Transparenz: Benutzerfeedback kann dazu beitragen, die Transparenz von KI-Bildgenerierungssystemen zu erhöhen. Indem Benutzer in den Entwicklungsprozess einbezogen werden, können KI-Entwickler Informationen darüber teilen, wie das System funktioniert, seine Grenzen und die mögliche Voreingenommenheit, die es haben kann. Transparenz ermöglicht es den Benutzern, ein besseres Verständnis dafür zu haben, wie ihre Daten verwendet werden und wie das KI-System Bilder generiert.

Verantwortlichkeit: Benutzerfeedback spielt eine entscheidende Rolle bei der Rechenschaftspflicht von KI-Entwicklern und -Systemen. Es ermöglicht den Benutzern, Bedenken hinsichtlich potenzieller ethischer Probleme, Voreingenommenheit oder unbeabsichtigter Konsequenzen im Zusammenhang mit den generierten Bildern zu äußern. Entwickler können dieses Feedback nutzen, um Mängel zu identifizieren und zu beheben, die Algorithmen zu verfeinern und die Verantwortung für die Auswirkungen ihrer KI-Systeme zu übernehmen.

Fairness: Benutzerfeedback hilft dabei, potenzielle Voreingenommenheit im KI-Bildgenerierungsprozess zu identifizieren. Unterschiedliche Benutzer können unterschiedliche Erwartungen, kulturelle Kontexte oder Sensibilitäten haben. Indem sie mit einer vielfältigen Benutzergruppe interagieren, können Entwickler Voreingenommenheit erkennen und mindern, die zu unfairer oder diskriminierender Bildgenerierung führen könnten. Benutzerfeedback bietet die Möglichkeit sicherzustellen, dass das KI-System eine breite Palette von Perspektiven berücksichtigt und verschiedene kulturelle und gesellschaftliche Normen respektiert.


\section{Technische Methoden}

Technische Ansätze sind solche, die von den Entwicklern einer IGAI sowohl während als auch nach der Entwicklungsphase eingesetzt werden können, beispielsweise in Form von Datenaufbereitung und -manipulation oder Filter. 
Aufgrund dessen werden in der weiteren Durchführung diese Ansätze danach gruppiert, ob diese in der Entwicklungsphase der IGAI oder in deren Anwendungsphase anzuwenden sind.

\subsection{Entwicklungsphase}

\subsubsection{Verwendung von Datenerweiterung (Data Augmentation)}
Data Augmentation, zu deutsch Datenerweiterung, beschreibt eine Technik, Varianz in vorhandene Daten einzufügen und so die Datenmenge zu erweitern. Dabei werden synthetische Daten aus vorhandenen Daten erzeugt, welche geringe Unterschiede zu den Originaldaten haben, oder aus Kombinationen mehrer Originaldaten \cite[S. 2]{Shorten}.
Im Fall von IGAIs könnten so Eingabetexte, mit denen ethisch kritische Inhalte erzeugt werden könnten, im mehreren leicht oder stark abgewandelten Formen der IGAI vorgegeben werden, um die Konsistenz dieser zu überprüfen und einen besseren Umgang mit kritischen Inhalten anzutrainieren.  

\subsubsection{Verwendung von überwachtem Lernen}
Nach dem im Kapitel \nameref{def_ai} vorgestellten Prinzip des überwachtem Lernens, könnte man für IGAIs Datensätze erstellen, welche aus 2-Tupeln bestehen: einem Text anhand dem ein Bild erzeugt werden soll und einem dazu passenden moralisch unbedenklichen Bild. Das Modell kann so anhand von positiven Beispielen lernen, welche Inhalte es erzeugen darf.

Auch das Gegenteil wäre möglich: ein geordnetes Paar an Texten und moralisch verwerflichen Bildern, anhand derer die KI lernen kann, was sie auf keinen Fall generieren darf.

\subsubsection{Verwendung von verstärktem Lernen}
Wie in \nameref{def_ai} erklärt, wird beim verstärkten Lernen ein Belohnungs-Bestrafungs-System eingesetzt. Dieses könnte im Fall von IGAIs daraus bestehen, die IGAI zu belohnen, wenn 
\begin{itemize}
    \item ethisch nicht vetretbare Texte erkannt werden und kein Bild erzeugt wird,
    \item ethisch vetretbare Texte erkannt werden und ein Bild erzeugt wird
\end{itemize}
oder zu bestrafen, wenn
\begin{itemize}
    \item ethisch nicht vetretbare Texte nicht erkannt werden und ein Bild erzeugt wird,
    \item ethisch vetretbare Texte erkannt werden, das erzeugte Bild aber nicht ethisch vetretbar ist.
\end{itemize}  

\subsubsection{Verwendung von kontradiktorischem Lernen}
Adversarial Learning, zu deutsch kontradiktorisches Lernen, ist eine Technik, die eingesetzt werden sollte, um einen Machine-Learning-Algorithmus ethisch hochwertiger zu gestalten. 

Dabei wird ein zusätzlicher Algorithmus, der als Adversary bezeichnet wird, verwendet, um den Hauptalgorithmus herauszufordern und zu verbessern. \cite[S. 3]{Kurakin}

Der Adversary wird darauf trainiert, gezielt ethisch problematische Beispiele zu generieren, die der Hauptalgorithmus möglicherweise falsch klassifizieren könnte.
Durch die Konfrontation des Hauptalgorithmus mit solchen herausfordernden Beispielen wird dieser dazu gezwungen, seine Entscheidungen präziser zu treffen und ethisch akzeptablere Ergebnisse zu erzeugen. Durch diesen iterativen Prozess des Trainings und der Herausforderung kann der Algorithmus lernen, ethische Aspekte besser zu berücksichtigen und so seine Leistung im Hinblick auf Ethik zu verbessern. 

\subsection{Anwendungsphase}
Die Anwendungsphase ist der Zeitraum, nachdem eine IGAI fertig entwickelt wurde bis zu dem Zeitpunkt, an dem ihr Dienst eingestellt wird. In diesem Zeitraum wird die IGAI von den Nutzern verwendet. Techniken in der Anwendungsphase nehmen keinen direkten Einfluss auf die Funktionsweise der IGAI selbst. Die Techniken verändern die Rahmenbedingungen in denen die IGAI verwendet wird.

\subsubsection{Input-Filter}
Die Entwickler von IGAIs müssen davon ausgehen, dass Nutzer unethische Eingaben vornehmen. Damit ist der Input in Textform gemeint, den die IGAI verwendet, um daraus Bilder zu generieren. Um zu verhindern, dass diese Eingaben vorgenommen werden, können die Entwickler Input-Filter verwenden. Die Autoren \cite{Shah} haben eine AI entwickelt, die unmoralischen Inhalt aus sozialen Netzwerken erkennen kann. Die Entwickler von IGAIs können anhand dieser Ergebnisse ihre eigene AI entwickeln, jedoch mit der Spezialisierung auf unethischen Inhalt anstatt unmoralischen Inhaltes. Diese AI können sie dann als Filter verwenden, um einzuschränken welche Eingaben überhaupt zur IGAI gelangen.

\subsubsection{Output-Filter}
Wenn eine IGAI Bilder generiert, müssen Entwickler davon ausgehen, dass dabei unethische Bilder generiert werden. Diese Bilder dürfen den Nutzer nicht erreichen. Um das zu verhindern können die Entwickler Output-Filter verwenden. Der Output ist das von der IGAI generierte Bild. Die Autoren \cite{Zheng} haben ein System entwickelt, das aus zwei getrennten Filtern besteht: dem verbotene Symbole Filter und dem nicht jugendfreie Bilder Filter. Da diese Filter unabhängig voneinander arbeiten, kann das System um weitere Filter ergänzt werden. Die Entwickler haben damit die Möglichkeit alles zu filtern, was dem Nutzer nicht ausgegeben werden soll. Das System der Autoren war schneller und zuverlässig als seine Vorgänger. In Anbetracht des Alters des Artikels, ist anzunehmen, dass heutige Systeme noch schneller und zuverlässiger den zu filternden Inhalt ermitteln. Ein solches System kann von den Entwicklern verwendet werden, um den Output zu überwachen. Erkennt das System ein generiertes Bild als unethisch wird es nicht an den Nutzer ausgegeben. Außerdem können diese Ergebnisse auch zur Weiterentwicklung der IGAI selbst verwendet werden.

\subsubsection{Anwendungsbeschränkung}
Anwendungsbeschränkung bedeutet, dass Entwickler die Art und Weise wie Nutzer mit der IGAI interagieren einschränken. In dieser Methode soll es um eine Beschränkung der Anwendungsfrequenz gehen. Als Beispiel dient hierbei das Credit-System von DALL E2 von OpenAI. Bei diesem System werden Credits benötigt, um mittels der IGAI ein Bild zu generieren. Credits sind käuflich oder werden gestatten, wenn man sich in einem bestimmten Zeitraum nach Veröffentlichung von DALL E2 angemeldet hat \cite{Jang}. Wenn die IGAI ein Bild generiert, wird ein Credit verbraucht. Hat ein Nutzer keine Credits mehr, kann er keine Bilder mehr generieren. Dadurch wird eine initiale Barriere geschaffen, da Nutzer Credits kaufen müssen, um Bilder zu generieren. Nutzer, die auf die Generierung von unethischen Bildern abzielen, können damit abgeschreckt werden. Durch Anzahl der Credits kann zusätzlich beschränkt werden wie viele Bilder generiert werden können. Entwickler können dieses Credit-System nach ihrem Bedarf anpassen.

\chapter{Fazit}
Das Ziel der vorliegenden Arbeit besteht darin, die Frage zu beantworten, welche Methoden es aktuell gibt, mit denen IGAI-Entwickler dazu beitragen können, dass mit ihrer Technologie keine unethischen Inhalte erzeugt werden. Diese Frage konnte durch die detaillierte Auflistung und Erklärung der Methoden in Kapitel BITTE QUERVERWEIS EINFÜGEN beantwortet werden. Die gewählte Methode, ausschließlich einen Überblick über die in der Literatur beschriebenen Methoden auszuführen, kann jedoch nicht zeigen, welche Methoden tatsächlich wie angewandt werden und ihren Zweck inwiefern erfüllen. Von den Autoren der vorliegenden Arbeit selbst erdachte Methoden wie die Verwendung von überwachtem Lernen in Kapitel BITTE QUERVERWEIS EINFÜGEN konnten nicht auf ihre tatsächliche Anwendbarkeit überprüft werden, da dies den Rahmen der vorliegenden Arbeit sprengen würde. Es würde sich im Anschluss an die vorliegende Arbeit also anbieten, experimentell die tatsächliche Umsetzung der genannten Methoden an verschiedenen IGAIs zu überprüfen bzw. auch experimentell zu untersuchen, inwiefern die skizzierten im Rahmen der vorliegenden Arbeit erdachten Methoden tatsächlich anwendbar und dienlich sind.
So kann diese Arbeit jedoch als Grundlage dienen, um einen Überblick über mögliche Methoden zu gewinnen, um darauf fußend ethisch korrekter handelnde KI zu entwickeln.

% Gefahr Achtung! Supervised Learning also überwachtes Lernen ist nicht selbst erdacht, ich hab nur die Quelle vergessen!

\bibliographystyle{ieeetr}
\bibliography{Sources.bib}
\end{document}
