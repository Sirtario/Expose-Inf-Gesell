\documentclass[12pt]{report}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{acronym}

\title{Erster Forschungsbericht \\[1ex] \large Methodiken zur Verhinderung von  Erzeugung unethischer Inhalte durch IGAI}
\date{21.05.2023}
\author{Ronja Drechsler, \and Dominic Fitter, \and Michel Hecker, \and Khaldon Kassem, \and Phillip Eckstein}

\begin{document}

\maketitle
\tableofcontents
\newpage
\begin{acronym}
    \acro{EU}{Europäische Union}
\acro{KI}{Künstliche Intelligenz}
\acro{AI}{Artificial Intelligence}
\acro{IGAI}{image generating artifical intelligence}
\acro{GAI}{Generative artifical intelligence}
\end{acronym}
\newpage
\chapter{Einleitung}

Die Generierung von Bildern mithilfe von KI reicht zurück bis in die 1960er Jahre\cite{Garcia}, doch erst in den letzten zwei Jahren wurden diese KIs durch die Veröffentlichungen von Projekten wie DALL-E (01/21), Midjourney (03/22) oder Stable Diffusion (08/22) der Allgemeinheit zugänglich gemacht. Auslotungen der Leistungsfähigkeit dieser KIs zeigen jedoch auf, dass sie sexistische oder rassistische Verzerrungen aufweisen \cite{Schmidt} und für unethische Zwecke missbraucht werden können, z. B. zur Erstellung von Deepfakes, pornografischen oder gewalttätigen Inhalten \cite{Hadero}.
Angesichts dessen haben u. a. die Entwickler von Midjourney reagiert und das Verwenden von Wörtern mit Bezug zur Thematik menschlicher Fortpflanzungssysteme verboten \cite{Heikkilae} und ihre kostenlose Testversion eingestellt \cite{NelsonMidjourney}. Daraus ergibt sich die Frage, welche Methodiken generell möglich sind bzw. angewandt werden, um die Erzeugung unethischer Inhalte durch \ac{IGAI} zu verhindern.
Die vorliegende Arbeit setzt sich mit dieser Forschungsfrage auseinander, indem sie zusammenträgt, welche Methoden in der Literatur beschrieben werden, um KIs vor Missbrauch zu schützen. Sie diskutiert, wie die Anbieter und Entwickler dieser Technologie verantwortungsbewusst handeln können, um sicherzustellen, dass ihre KIs nur für ethisch vertretbare Zwecke eingesetzt werden. Das Ziel besteht darin, aufzuzeigen wie \ac{IGAI}-Entwickler dazu beitragen können, dass mit ihrer Technologie keine unethischen Inhalte erzeugt werden.

% Bitte bei allen Zitationen an Seitenzahlen denken, also z. B., auf welcher Seite Schmidt das sagt. -> Das sind hier alles nur Nachrichtenartikel, haben keine Seitenzahl

\chapter{Definitionen}
Im Folgenden wird ein kurzer Abriss gegeben, wie sich der Begriff der Ethik bezüglich bildgenerierender KI in den letzten vier Jahren auf internationaler und nationaler Ebene entwickelt haben und welche Fachtermini aus der Thematik bildgenerierende KI in der vorliegenden Arbeit wie verwendet werden.
\section{Ethik}

\paragraph{UNESCO\cite{UNESCO}\\}
Im November 2019 verabschiedete die Generalversammlung der Organisation der Vereinten Nationen für Bildung, Wissenschaft und Kultur (\textbf{UNESCO}) auf ihrer 40. Sitzung die 40 C/Resolution 37, durch die sie die Generaldirektorin beauftragte, \glqq ein international gültiges Instrument zur Festlegung von Standards in Bezug auf die Ethik der künstlichen Intelligenz (KI) in Form einer Empfehlung vorzubereiten\glqq{}.\\Im November 2021 wurde auf der 41. Sitzung der \textbf{UNESCO} diese Empfehlung von den Mitgliedstaaten als weltweit erste globale Norm für die Ethik der künstlichen Intelligenz angenommen.\\

Diese Empfehlungen basieren auf allgemeingültigen Werten und Grundsätzen wie der Achtung der Menschenrechte, der Grundfreiheiten und der Rechtsstaatlichkeit in der digitalen Welt ebenso wie in der realen Welt. Das soll die Mitgliedstaaten ermutigen, diese Empfehlung freiwillig bei der Nutzung künstlicher Intelligenz einzuhalten und auch als Umsetzung ihres nationales Rechts zu begreifen. Im Detail werden folgende Werte in der Empfehlung angewandt:
%Zitation!

\paragraph{Werte:}
\begin{itemize}
	\item Respect, protection and promotion of human rights and fundamental freedoms and human dignity
	\item Environment and ecosystem flourishing
	\item Ensuring diversity and inclusiveness
	\item Living in peaceful, just and interconnected societies
\end{itemize}

Außerdem spiegeln sich diese Grundsätze wider:

\paragraph{Grundsätze:}
\begin{itemize}
	\item  Proportionality and Do No Harm
	\item  Safety and security
	\item  Fairness and non-discrimination
	\item  Sustainability
	\item  Right to Privacy, and Data Protection
	\item  Human oversight and determination
	\item  Transparency and explainability
	\item  Responsibility and accountability
	\item  Awareness and literacy
	\item  Multi-stakeholder and adaptive governance and collaboration
\end{itemize}
\paragraph{EU\cite{EUCommision}\\}
Im April 2019 veröffentlichte die Kommission der Europäischen Union \cite{EUCommision}.
% Bitte "(\textbf{EU}) \glqq ETHIK-LEITLINIEN FÜR EINE VERTRAUENSWÜRDIGE KI\glqq{}.\\" vollständig durch korrekte Zitation ersetzen, also so ähnlich wie "Im April 2019 veröffentlichte die Kommission der Europäischen Union [Kommission der Europäischen Union, 2019]". -> Hoffe das passt jetzt so
Demnach muss eine KI während ihres gesamten Lebenszyklus drei Komponenten erfüllen, um vertrauenswürdig zu sein: Sie muss a) rechtmäßig, b) ethisch und c) robust sein. Diese Komponenten bedienen die nachstehenden Grundsätze und Werte und geben ihre Umsetzung bei der Entwicklung, Einführung und Verwendung von KI-Systemen vor:

\paragraph{Werte:}
\begin{itemize}
	\item  Verhältnismäßigkeit und Schadensvermeidung
	\item  Sicherheit und Schutz
	\item  Fairness und Nichtdiskriminierung
	\item  Nachhaltigkeit
	\item  Recht auf Privatsphäre und Datenschutz
	\item  Menschliche Aufsicht und Entscheidungsbefugnis
	\item  Transparenz und Erklärbarkeit
	\item  Verantwortung und Rechenschaftspflicht
	\item  Bewusstsein und Bildung
	\item Multi-Stakeholder und adaptive Governance und Zusammenarbeit
\end{itemize}

\paragraph{Grundsätze:}
\begin{itemize}
	\item  Achtung der menschlichen Autonomie
	\item  Schadensverhütung
	\item  Fairness
	\item  Erklärbarkeit
\end{itemize}

Aufbauend auf diesen Grundsätzen wird vertrauenswürdige KI mithilfe eine Liste von sieben Anforderungen umgesetzt und verwirklicht, die erfüllt und durch sowohl technische und nicht-technische Ansätze während des gesamten Lebenszyklus eines KI-Systems umgesetzt werden müssen.

Die sieben Anforderungen lauten wie folgt:

\paragraph{Anforderungen an eine vertrauenswürdige KI:}
\begin{enumerate}
	\item  Vorrang menschlichen Handelns und menschliche Aufsicht
	\item  Technische Robustheit und Sicherheit
	\item  Schutz der Privatsphäre und Datenqualitätsmanagement
	\item  Transparenz
	\item  Vielfalt, Nichtdiskriminierung und Fairness
	\item  Gesellschaftliches und ökologisches Wohlergehen
	\item  Rechenschaftspflicht
\end{enumerate}

Seit April 2021 liegt ein Vorschlag der Europäischen Kommission vor, einheitliche Regeln für künstliche Intelligenz festzulegen, der einige Rechtssysteme in der Union ändern soll \cite{GesetzesentwurfEUComm}.

Vor dem Hintergrund dieses Vorschlags haben die Regierungen einiger Länder, wie die der Bundesrepublik Deutschland, versucht unter Berücksichtigung der Studie des Deutschen Komitees für die UNESCO in \cite{UNESCO} zu prüfen, ob in Bezug auf die diesbezüglichen Empfehlungen der UNESCO weiterer Handlungsbedarf besteht \cite{Deutscher_Bundestag}.  
%habe ich den Zitierzusammenhang mit dem Bundestag richtig verstanden oder ist das jetzt falsch so? -> ich denke das passt dann jetzt so


\section{KI}\label{def_ki}
Generative artifical intelligence (GAI) können neue Daten generieren, im Gegensatz zu vor deren Einführung entwickelten \ac{KI}s (im Englischen \ac{AI}), die nur Daten analysieren oder basierend auf gegebenen Daten zu handeln. Außerdem werden ihnen zum Trainieren enorme Datenmengen wie der komplette Inhalt von Wikipedia zur Verfügung gestellt. Daher sind diese \ac{AI}s erst mit steigender Rechenkapazität möglich geworden. \ac{GAI}s modellieren Netze aus miteinander verbundenen Daten in verschiedenen multimedialen Formaten. Demnach bestünden beispielsweise Verbindungen zwischen dem Wort Panda, ein Bild von einem Panda und einem Video von einem Panda. Dadurch können \ac{GAI}s ein beliebiges Eingabeformat in ein beliebiges Ausgabeformat übersetzen. Thema dieser Arbeit sind image generating artifical intelligence (IGAI) oder auch text-to-image \ac{AI}s, eine Unterkategorie von \ac{GAI}s. Diese \ac{AI}s können nach Eingabe von natürlichsprachlichem Text Bilder erzeugen.
\cite{Roberto}

%bitte hier nochmal über Konkretisierung von "steigender Rechenkapazität" nachdenken -> kommt so aus Quelle, laut Michel
% bitte nochmal überlegen, wie man beide schwammigen "bisher" auflösen kann -> kommt so aus Quelle, laut Michel
%bitte hier nochmal über Zitation o. ä. nachdenken -> kommt so aus Quelle, laut Michel

Um \ac{GAI}s zu trainieren, werden drei Arten von Machine Learning verwendet: überwachtes Lernen, unüberwachtes Lernen und verstärktes Lernen. 
Beim überwachten Lernen werden gekennzeichnete Datensätze verwendet, um Vorhersagen zu treffen. Die gekennzeichneten Daten werden als Trainingsdaten verwendet und nach dem Ausführen des Algorithmus wird überprüft, ob die getroffene Vorhersage der Kennzeichnung entspricht.
Beim unüberwachten Lernen wird dem Algorithmus nicht gekennzeichnete Daten zugeführt. Hierbei wird nicht versucht eine Vorhersage zu treffen, sondern Schlussfolgerungen oder Zusammenhänge zwischen den Daten zu finden.
Beim verstärkten Lernen wird ein System aus Belohnung und Strafen verwendet. Macht der Algorithmus wenige Fehler ist die Belohnung groß und die Strafe klein, macht der Algorithmus viele Fehler ist die Belohnung klein und die Strafe groß. Dadurch kann das Finden des 
optimalen Verhaltens automatisiert werden.
\cite{serafeim}

\include{StandForschung.tex}
\include{Methodik.tex}

\chapter{Durchführung}\label{execution}
Verschiedene Formen der Voreingenommenheit von AI lassen sich durch unterschiedliche Methoden verhindern. Nachfolgend werden die Methoden erläutert, wobei sie in technische und nicht-technische unterteilt werden.

\section{Nicht-Technische Methoden}
Nicht-technische Methoden sind solche, die nicht primär auf technische Lösungen setzen.\\
Die hiernach beschriebenen Methoden wurden, wenn keine andere Quelle zitiert wird, aus \cite{UNESCO} und \cite{EUCommision} abgeleitet. %bitte schauen, ob das mit dem Zitierstil so passt, ich will einfach nur, dass da so etwas wie "...aus [Fitter, 2022] und [Kassem, 2023] abgeleitet" steht. -> wird so schon passen

\subsection{Datenverzerrungen im Trainingsdatensatz}
Bereits bei der Zusammenstellung des Trainingsdatensatzes einer KI kann es zu Verzerrungen kommen. Würde eine KI durch einen so verzerrten Datensatz trainiert werden, könnte sich bei der Anwendung der KI diese Verzerrung wiederspiegeln. Eine antrainierte Verzerrung lässt sich, wenn überhaupt, nur sehr aufwendig wieder korrigieren, es empfiehlt sich also, von vornherein einen geeigneten Datensatz zu verwenden.

Dabei wird zwischen Stichprobenverzerrungen, Messwertverzerrungen, Kennzeichnungsverzerrungen oder Verzerrung durch Einseitigkeit unterschieden \cite[S. 48ff.]{Srinivasan}.
Beispiele für Stichprobenverzerrungen können sein, dass zu wenig Daten in die Datensätze aufgenommen werden und diese keine realitätsnahe Darstellung ermöglichen.
Auch ist es möglich, dass die Daten systematisch einseitig sind. Ein Gesichtserkennungsmodell, das hauptsächlich an hellhäutigen Gesichtern trainiert wurde, kann z. B. bei dunkleren Hauttönen schlechter abschneiden oder bestimmte Gesichtsformen und -farben gar nicht als Gesichter erkennen.
Zudem ist eine systematisch Messwertverzerrung möglich, etwa wenn Bilder, die mit einer nicht vollständig funktionstüchtigen Kamera aufgenommen worden, in die Trainingsdaten einbezogen werden.
Es kann auch vorkommen, dass Daten falsch, überflüssig oder unzuverlässig gekennzeichnet werden. So ist die Unterscheidung zwischen \textit{Wiese} und \textit{Gras} selten nötig. Existieren allerdings beide Label im Trainingsdatensatz einer KI, kann sich dies negativ auf die Genauigkeit des Modells auswirken, wenn bei der Beschriftung nicht konsequent beide Bezeichnungen unterschieden werden\cite[S. 48ff.]{Srinivasan}. 

Eine Überwachungsinstanz in Form eines neutralen Beobachters oder Systems zuzüglich einer reflektierten Selbstkontrolle der Datensammler kann dabei helfen, solche Verzerrungen zu vermeiden.
Bei dieser Überwachung sollten mindestens nachstehende Überlegungen Beachtung finden: %Haben wir uns die ausgedacht oder müssen wir sie zitieren? -> ausgedacht
\begin{itemize}

    \item Repräsentativität: Die Daten müssen die Realität widerspiegeln und nicht nur Randgruppen oder Mehrheiten darstellen.
    \item Quantität: Die Anzahl der Daten muss ausreichen,  um eine Wiederspiegelung der Realität gewährleisten zu können.
    \item Qualität: Die Kennzeichnung der Daten muss auf Voreingenommenheit und Redundanz geprüft werden.
    \item Güte: Die Daten müssen qualitativ hochwertig, mindestens aber für den Anwendungsfall ausreichend sein.

\end{itemize}

\subsection{Verzerrungen in der Anforderungsformulierungen}
Bei den Definitionen einer Aufgabenstellung und der Anforderungen an ein Modell kann es zu Verzerrungen kommen. Diese Verzerrung kann einerseits durch die Formulierung der Aufgabenstellung aus den Anforderungen oder durch die Art der verwendeten Daten entstehen.  \cite[S. 51f.]{Srinivasan} 

Ein Problem bei der Formulierung der Anforderungen an einen Algorithmus kann beispielsweise darin bestehen, wenn Daten wie Geschlecht, Hautfarbe oder ethnische Zugehörigkeit in die Entscheidung einfließen, ob einer Person ein Kredit gewährt wird. Hierdurch können sexistische oder rassistische Diskriminierungen bei der Entscheidung stattfinden \cite[S. 51f.]{Srinivasan}.

Es empfiehlt sich das Einsetzten eines neutralen Beobachters, welcher die Anforderungen und daraus entstehende Aufgabenstellung auf folgende Aspekte prüft: %Haben wir uns die ausgedacht oder müssen wir sie zitieren? -> ausgedacht
\begin{itemize}
    \item Bei dem Treffen einer Entscheidung des Modells dürfen keine Aspekte berücksichtigt werden, die für eine erfolgreiche Ergebnisfindung der Aufgabenstellung nicht nötig sind.
    \item Die Formulierung muss neutral gestaltet sein, sodass sich durch die Formulierung an sich keine Schlüsse ziehen lassen, die mit Voreingenommenheit einhergehen.
\end{itemize}

\subsection{Vermeidung von Verzerrungen bei der Modellbewertung}
Neben der Möglichkeit, dass die Trainingsdaten oder der Algorithmus Verzerrungen enthalten, kann es auch sein, dass die Evaluation des Modells Verzerrungen aufweist.
Diese können durch menschliches Verhalten der Tester, oder durch schlecht entworfene Test-Frameworks auftreten \cite[S. 54f.]{Srinivasan}.

Menschliche Bewerter eines Modells können auf verschiedene Arten unethisch handeln. Sie können durch eigene Einstellungen und Erfahrungen beeinflusst sein, auch unbeabsichtigt.
Gerade wenn ein Bewerter direkt am Model beteiligt ist, kann es zum sogenannten Bestätigungsfehler, oder \glqq Peak-End-Effect\grqq{} kommen. Ein Bestätigungsfehler ergibt sich aus der Neigung, durch subjektive Wahrnehmung eine Erwartung bestätigt zu sehen. Ist so beispielsweise der Beurteiler beruflich oder finanziell direkt vom Erfolg des Projekts abhängig ist es eher möglich, dass Fehler nicht auffallen oder relativiert werden, als wenn er es nicht wäre.
Beim \glqq Peak-End-Effect\grqq{} wird eine Bewertung anhand von einzelnen Besonderheiten oder dem letzten Eindruck geprägt.
Ferner kann ein entworfenes Test-Framework fehlerhaft sein, wenn darin zum Beispiel zu wenig getestet wird, oder mit nicht geeigneten Testdaten gearbeitet wird \cite[S. 54f.]{Srinivasan}.

Um ein Modell ausreichend zu testen sollten einige Punkte berücksichtigt werden: %Haben wir uns die ausgedacht oder müssen wir sie zitieren? -> ausgedacht
\begin{itemize}
    \item Wenn Menschen das Modell bewerten, sollten diese nicht direkt an der Entwicklung beteiligt sein, um Neutralität zu gewährleisten.
    \item Wenn das Modell durch Menschen evaluiert wird, sollte diese Evalutaion von mehreren Personen mit unterschiedlichen Hintergründen durchgeführt werden.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses mindestens eine für den Anwendungsfall annehmbare Menge an voneinander unabhängigen Tests durchführen.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses weitreichend auf systematische Fehler überprüft werden.
\end{itemize}


\subsection{Abgrenzung des rechtlichen Rahmens durch AGBs}
Eine Etablierung von allgemeinen Geschäftsbedingungen (AGBs) legt den rechtlichen Rahmen für die Nutzung von \ac{IGAI}s fest. Dadurch können sie dazu beitragen, den Missbrauch solcher Systeme einzudämmen. 
Allerdings sind AGBs allein nicht hinreichend, um den Missbrauch von \ac{IGAI}s gänzlich zu verhindern, da sie keine aktive Nutzungsbeschränkung bieten. Wesentliche Aspekte könnten sein:
\begin{itemize}
    \item Festlegung des Nutzungszwecks: \ac{AGB}s sollten explizit den beabsichtigten Nutzungszweck der \ac{KI} definieren. Dadurch werden ein Rahmen für akzeptable Aktivitäten abgesteckt und unerwünschte Nutzungsformen ausgeschlossen.
    \item Untersagung unzulässiger Aktivitäten: \ac{AGB}s sollten eine Liste von Aktivitäten enthalten, die als Missbrauch der \ac{KI} gelten und somit untersagt sind.
    \item Haftungsausschluss: Ein Haftungsausschluss in den \ac{AGB}s ist bedeutsam, um zu verdeutlichen, dass der Anbieter der \ac{KI} nicht für den Missbrauch der Technologie durch die Nutzer verantwortlich gemacht werden kann. Dadurch sollen potenzielle rechtliche Konsequenzen für den Anbieter eingeschränkt werden.
    \item Festlegung von Nutzungsregeln: Die \ac{AGB}s  können spezifische Regeln für die Nutzer der \ac{KI} enthalten, um den Missbrauch einzudämmen. Dies könnte beispielsweise die Verpflichtung zur Registrierung, die Verwendung sicherer Zugangsdaten oder die Einhaltung ethischer Richtlinien umfassen.
    \item Überwachung und Sanktionen: \ac{AGB}s sollten auch klare Informationen darüber bereitstellen, wie die Nutzung der \ac{KI} überwacht wird und welche Sanktionen im Falle von Missbrauch verhängt werden können. Dies kann beispielsweise die Möglichkeit zur Überprüfung von Nutzeraktivitäten, die Sperrung von Konten oder die Meldung rechtswidrigen Verhaltens umfassen.
\end{itemize}

\subsection{Zusammenarbeit und Feedback}
Nutzerfeedback und Engagement beinhalten das aktive Einbeziehen von Nutzern während des Entwicklungsprozesses und die Integration ihrer Perspektiven, Bedenken und Werte in die Gestaltung und Entscheidungsfindung von \ac{KI}-Systemen. Durch die Berücksichtigung des Nutzerfeedbacks können \ac{KI}-Entwickler vielseitigere Einblicke in die potenziellen ethischen Implikationen ihrer Bildgenerierungsmodelle gewinnen und sicherstellen, dass die generierten Bilder mit den Erwartungen und Werten der Nutzer übereinstimmen.

Durch eine aktive Zusammenarbeit mit externen Interessengruppen, Forschern und der breiteren Gemeinschaft können Organisationen tiefgründigere Erkenntnisse sammeln, Bedenken ansprechen und ihre Praktiken kontinuierlich verbessern. Dieser kollaborative Ansatz gewährleistet, dass die Entwicklung und Bereitstellung von \ac{IGAI}-Technologien mit gesellschaftlichen Werten und ethischen Standards im Einklang stehen.\cite{EUCommision}\cite{UNESCO}

\subsubsection{Partnerschaften über Sektorengrenzen hinweg}
Zusammenarbeit und Partnerschaften mit anderen Institutionen einschließlich akademischen Forschern, gemeinnützigen Organisationen, Regierungsbehörden und Branchenkollegen ermöglichen eine breitere Perspektive und ein umfassenderes Verständnis der ethischen Implikationen der Bildgenerierung mittels \ac{KI}. Durch die Nutzung des gemeinsamen Fachwissens und der Ressourcen verschiedener Sektoren können Organisationen komplexe Herausforderungen angehen und einen ganzheitlicheren Ansatz für ethische KI sicherstellen.\cite{Vogel}

\subsubsection{Partizipative Gestaltung}
Indem die Endnutzer und Stakeholder in den Design- und Entwicklungsprozess von \ac{IGAI} einbezogen werden, können die Entwickler nach \cite{Zytko} vielseitigere Einblicke gewinnen, spezifische Bedürfnisse identifizieren und Lösungen gemeinsam entwickeln, die den Werten der Nutzer und ethischen Standards entsprechen. Partizipative Designmethoden wie nutzerzentriertes Design und Co-Creation-Workshops fördern die Zusammenarbeit und Empathie und stellen sicher, dass \ac{KI}-Systeme unter Berücksichtigung der Nutzerinteressen gestaltet werden.

\subsubsection{Öffentliche Beteiligung und Bewusstseinsbildung}
Laut \cite{WILSON2022101652} kann eine Förderung der öffentlichen Beteiligung an und Bewusstseinsbildung zu ethischen Aspekten bei der Anwendung von \ac{KI} dienlich sein, um eine breitere Palette von Interessengruppen in den Dialog über die Ethik von Bildgenerierung durch \ac{KI} einzubeziehen. Im Detail werden öffentliche Konsultationen, Workshops und öffentliche Debatten über die ethischen Implikationen von KI-Technologien empfohlen. Solche Initiativen schaffen \cite{WILSON2022101652} zufolge Raum für Diskussionen, fördern diverse Perspektiven und befähigen Einzelpersonen, zur Entwicklung ethischer Leitlinien und Richtlinien beizutragen.

\subsubsection{Kontinuierliche Überwachung und Evaluation}
Potenzielle Voreingenommenheit, unbeabsichtigte Folgen oder aufkommende ethische Fragen können gemäß \cite{EUCommision} durch die Etablierung fortlaufender Überwachungs- und Evaluierungsprozesse zur Bewertung der ethischen Auswirkungen und Leistung von \ac{IGAI}-Systemen identifiziert werden. Dies bedeute konkret eine regelmäßige Erfassung und Analyse von Rückmeldungen von Nutzern, betroffenen Gemeinschaften und anderen Interessengruppen. Eine solche Feedbackschleife ermögliche es Organisationen, Bedenken proaktiv anzugehen, ihre Modelle zu überarbeiten und sicherzustellen, dass ethische Praktiken während des gesamten Lebenszyklus von \ac{IGAI}-Systemen eingehalten werden.

\subsubsection{Nutzerfeedback}
In \cite{Mittelstadt} wird argumentiert, dass Nutzerfeedback und -beteiligung zu mehreren ethischen Dimensionen von \ac{KI}-Algorithmen beitragen können, einschließlich Transparenz, Verantwortlichkeit und Fairness. Dieses Konzept könnte um folgende Aspekte erweitert werden:

%muss das Folgende nicht als Aufzähungsliste formatiert werden? -> ist jetzt ne aufzählung
\begin{itemize}
	\item Transparenz: Nutzerfeedback kann dazu beitragen, die Transparenz von \ac{IGAI}-Systemen zu erhöhen. Indem Nutzer in den Entwicklungsprozess einbezogen werden, können KI-Entwickler Informationen über die Funktion des Systems, seine Grenzen und die mögliche Voreingenommenheit teilen, die es haben kann. Transparenz ermöglicht den Nutzern ein besseres Verständnis dafür, wie ihre Daten verwendet werden und wie das \ac{KI}-System Bilder generiert.
	\item Verantwortlichkeit: Nutzerfeedback spielt eine entscheidende Rolle bei der Rechenschaftspflicht von \ac{KI}-Entwicklern und -Systemen. Es ermöglicht den Nutzern, Bedenken hinsichtlich potenzieller ethischer Probleme, Voreingenommenheit oder unbeabsichtigter Konsequenzen im Zusammenhang mit den generierten Bildern zu äußern. Entwickler können dieses Feedback nutzen, um Mängel zu identifizieren und zu beheben, die Algorithmen zu verfeinern und die Verantwortung für die Auswirkungen ihrer \ac{KI}-Systeme zu übernehmen.
	\item Fairness: Nutzerfeedback hilft dabei, potenzielle Voreingenommenheit im \ac{IGAI}-Prozess zu identifizieren. Unterschiedliche Nutzer können unterschiedliche Erwartungen, kulturelle Kontexte oder Sensibilitäten haben. Indem sie mit einer vielfältigen Nutzergruppe interagieren, können Entwickler Voreingenommenheit erkennen und mindern, die zu unfairer oder diskriminierender Bildgenerierung führen könnten. Nutzerfeedback bietet die Möglichkeit sicherzustellen, dass das KI-System eine breite Palette von Perspektiven berücksichtigt und verschiedene kulturelle und gesellschaftliche Normen respektiert.
\end{itemize}


\section{Technische Methoden}

Technische Ansätze können von den Entwicklern einer \ac{IGAI} sowohl während als auch nach der Entwicklungsphase eingesetzt werden, beispielsweise in Form von Datenaufbereitung und -manipulation oder Filtern. Aufgrund dessen werden in der weiteren Durchführung diese Ansätze danach gruppiert, ob diese in der Entwicklungsphase der \ac{IGAI} oder in deren Anwendungsphase anzuwenden sind.

\subsection{Entwicklungsphase}

\subsubsection{Verwendung von Datenerweiterung (Data Augmentation)}
Datenerweiterung beschreibt eine Technik, Varianz in vorhandene Daten einzufügen und so die Datenmenge zu erweitern. Dabei werden synthetische Daten aus vorhandenen Daten erzeugt, die geringe Unterschiede zu den Originaldaten haben, oder aus Kombinationen mehrerer Originaldaten entstehen \cite[S. 2]{Shorten}. %Falls Shorten nur diese Erzeugung synthetischer Daten erklärt, stimmt das so, wenn er sich allerdings die konkrete, in den folgenden Sätzen beschriebene Methode ausgedacht hat, bitte wie in den oberen Abschnitten einflechten, sodass klar ist, dass alles von ihm stammt. -> stammt von Dominics Kopf, der ist wenn Schuld
Im Fall von \ac{IGAI}s könnten so Eingabetexte, mit denen ethisch kritische Inhalte erzeugt werden könnten, in mehreren leicht oder stark abgewandelten Formen der IGAI als Input-Text 
gegeben und anschließend die erzeugten Bilder überprüft werden, um die Konsistenz dieser zu überprüfen und einen besseren Umgang mit kritischen Inhalten anzutrainieren.  

\subsubsection{Verwendung von überwachtem Lernen}\label{Supervised Learning}
Nach dem im Kapitel \nameref{def_ai} vorgestellten Prinzip des überwachten Lernens könnte man für \ac{IGAI}s Datensätze erstellen, welche aus 2-Tupeln bestehen: einem Text anhand dem ein Bild erzeugt werden soll und einem dazu passenden ethisch unbedenklichen Bild. Das Modell kann so anhand von positiven Beispielen lernen, welche Inhalte es erzeugen darf.

Auch das Gegenteil wäre möglich: ein geordnetes Paar an Texten und ethisch verwerflichen Bildern, anhand derer die KI lernen kann, was sie auf keinen Fall generieren darf.

\subsubsection{Verwendung von verstärktem Lernen}
Wie in \nameref{def_ai} erklärt, wird beim verstärkten Lernen ein Belohnungs-Bestrafungs-System eingesetzt. Dieses könnte im Fall von \ac{IGAI}s daraus bestehen, die \ac{IGAI} zu belohnen, wenn 
\begin{itemize}
    \item ethisch nicht vetretbare Texte erkannt werden und kein Bild erzeugt wird,
    \item ethisch vetretbare Texte erkannt werden und ein Bild erzeugt wird
\end{itemize}
oder zu bestrafen, wenn
\begin{itemize}
    \item ethisch nicht vetretbare Texte nicht erkannt werden und ein Bild erzeugt wird,
    \item ethisch vetretbare Texte erkannt werden, das erzeugte Bild aber nicht ethisch vetretbar ist.
\end{itemize}  

\subsubsection{Verwendung von kontradiktorischem Lernen (Adversarial Learning)}
Ein Machine-Learning-Algorithmus kann durch Anwendung von kontradiktorischem Lernen ethisch hochwertiger gestaltet werden. Dabei wird ein als Adversary bezeichneter zusätzlicher Algorithmus verwendet, um den Hauptalgorithmus herauszufordern und zu verbessern. \cite[S. 3]{Kurakin} Der Adversary wird darauf trainiert, gezielt ethisch problematische Beispiele zu generieren, die der Hauptalgorithmus möglicherweise falsch klassifizieren könnte. Durch die Konfrontation des Hauptalgorithmus mit solchen herausfordernden Beispielen wird dieser dazu gezwungen, seine Entscheidungen präziser zu treffen und ethisch akzeptablere Ergebnisse zu erzeugen.

\subsection{Anwendungsphase}
Die Anwendungsphase ist der Zeitraum von der Fertigstellung einer \ac{IGAI} bis zu dem Zeitpunkt, an dem ihr Dienst eingestellt wird. In diesem Zeitraum wird die \ac{IGAI} von den Nutzern verwendet. Techniken in der Anwendungsphase nehmen keinen direkten Einfluss auf die Funktionsweise der \ac{IGAI} selbst. Die Techniken verändern die Rahmenbedingungen in denen die \ac{IGAI} verwendet wird.

\subsubsection{Input-Filter}
Die Entwickler von \ac{IGAI}s müssen davon ausgehen, dass Nutzer unethische Eingaben vornehmen. Damit ist der Input in Textform gemeint, den die \ac{IGAI} verwendet, um daraus Bilder zu generieren. Um zu verhindern, dass diese Eingaben vorgenommen werden, können die Entwickler Input-Filter verwenden. In \cite{Shah} wird eine \ac{KI} beschrieben, die unmoralischen Inhalt aus sozialen Netzwerken erkennen kann. Die Entwickler von \ac{IGAI}s können anhand dieser Ergebnisse ihre eigene \ac{KI} entwickeln, jedoch mit der Spezialisierung auf unethischen Inhalt anstatt unmoralischen Inhaltes. Diese \ac{AI} können sie dann als Filter verwenden, um einzuschränken welche Eingaben überhaupt zur \ac{IGAI} gelangen.

\subsubsection{Output-Filter}
Wenn eine \ac{IGAI} Bilder generiert, müssen Entwickler davon ausgehen, dass dabei unethische Bilder generiert werden können. Diese Bilder dürfen den Nutzer nicht erreichen. Um das zu verhindern können die Entwickler Output-Filter verwenden. Der Output ist das von der \ac{IGAI} generierte Bild. \cite{Zheng} beschreibt ein System, das aus zwei getrennten Filtern besteht: einem Filter für verbotene Symbole und einem für nicht jugendfreie Bilder. Da diese Filter unabhängig voneinander arbeiten, kann das System um weitere Filter ergänzt werden. Dies ermöglicht eine Filterung aller Inhalte, die einem Nutzer der \ac{KI} nicht ausgegeben werden sollen. Das beschriebene System ist laut \cite{Zheng} schneller und zuverlässig als seine Vorgänger. In Anbetracht des Alters des Artikels ist anzunehmen, dass heutige Systeme noch schneller und zuverlässiger den zu filternden Inhalt ermitteln. Ein solches System kann von den Entwicklern verwendet werden, um den Output zu überwachen. Erkennt das System ein generiertes Bild als unethisch wird es nicht an den Nutzer ausgegeben. Außerdem können diese Ergebnisse auch zur Weiterentwicklung der \ac{IGAI} selbst verwendet werden.

\subsubsection{Anwendungsbeschränkung} %Haben wir uns das selbst ausgedacht oder ist das alles von Jang? Ich habe es mal so geschrieben, als käme alles von Jang, wenn nicht, bitte wieder zurückändern. -> Eigenes Beipspiel anhand von Jang, brauch also keine Quelle
Anwendungsbeschränkung bedeutet, dass Entwickler die Art und Weise wie Nutzer mit der \ac{IGAI} interagieren einschränken. In dieser Methode soll es um eine Beschränkung der Anwendungsfrequenz gehen, wie sie \cite{Jang} beschreibt. Als Beispiel dient hierbei das Credit-System von DALL E2 von OpenAI. Bei diesem System werden Credits benötigt, um mittels der \ac{IGAI} ein Bild zu generieren. Credits sind käuflich oder werden gestatten, wenn man sich in einem bestimmten Zeitraum nach Veröffentlichung von DALL E2 angemeldet hat. Wenn die \ac{IGAI} ein Bild generiert, wird ein Credit verbraucht. Hat ein Nutzer keine Credits mehr, kann er keine Bilder mehr generieren. Dadurch wird eine initiale Barriere geschaffen, da Nutzer Credits kaufen müssen, um Bilder zu generieren. Nutzer, die auf die Generierung von unethischen Bildern abzielen, können damit abgeschreckt werden. Durch Anzahl der Credits kann zusätzlich beschränkt werden wie viele Bilder generiert werden können. Entwickler können dieses Credit-System nach ihrem Bedarf anpassen.

\chapter{Fazit}
Das Ziel der vorliegenden Arbeit besteht darin, die Frage zu beantworten, welche Methoden es aktuell gibt, mit denen \ac{IGAI}-Entwickler dazu beitragen können, dass mit ihrer Technologie keine unethischen Inhalte erzeugt werden. Diese Frage konnte durch die detaillierte Auflistung und Erklärung der Methoden in Kapitel \nameref{execution} beantwortet werden. Die gewählte Methode, ausschließlich einen Überblick über die in der Literatur beschriebenen Methoden auszuführen, kann jedoch nicht zeigen, welche Methoden tatsächlich wie angewandt werden und ihren Zweck inwiefern erfüllen. Von den Autoren der vorliegenden Arbeit selbst übertragene Methoden wie die Verwendung von überwachtem Lernen in Kapitel \nameref{Supervised Learning} konnten nicht auf ihre tatsächliche Anwendbarkeit überprüft werden, da dies den Rahmen der vorliegenden Arbeit sprengen würde. Es würde sich im Anschluss an die vorliegende Arbeit also anbieten, experimentell die tatsächliche Umsetzung der genannten Methoden an verschiedenen \ac{IGAI}s zu überprüfen bzw. auch experimentell zu untersuchen, inwiefern die skizzierten im Rahmen der vorliegenden Arbeit erdachten Methoden tatsächlich anwendbar und dienlich sind.
So kann diese Arbeit jedoch als Grundlage dienen, um einen Überblick über mögliche Methoden zu gewinnen, um darauf fußend ethisch korrekter handelnde \ac{KI} zu entwickeln.

% Gefahr Achtung! Supervised Learning also überwachtes Lernen ist nicht selbst erdacht, ich hab nur die Quelle vergessen! -> habs zu übertragen geändert

\bibliographystyle{ieeetr}
\bibliography{Sources.bib}
\end{document}