\documentclass[12pt]{report}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage[acronym]{glossaries}
\usepackage[left = 3.5cm, right = 1.5cm]{geometry}


\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter.}{0.5em}{}

\usepackage{enumitem}
\setlist{nosep}
\titlespacing*{\chapter}{0pt}{0pt}{12pt}
\titlespacing*{\section}{0pt}{16pt}{8pt}
\titlespacing*{\subsection}{0pt}{12pt}{6pt}
\titlespacing*{\subsubsection}{0pt}{8pt}{4pt}

\title{Erster Forschungsbericht \\[1ex] \large Methodiken zur Verhinderung von  Erzeugung unethischer Inhalte durch IGAI}
\date{21.05.2023}
\author{Ronja Drechsler, \and Dominic Fitter, \and Michel Hecker, \and Khaldon Kassem, \and Phillip Eckstein}

\makenoidxglossaries
\newglossaryentry{ai}
{
    name=ai,
    description={}
}

\newglossaryentry{igai}
{
    name=igai,
    description={image generatin atrificial intelligence}
}
\newglossaryentry{gai}
{
    name=gai,
    description={}
}
\newglossaryentry{tos}
{
    name=tos,
    description={}
}
\newglossaryentry{bias}
{
    name=bias,
    description={}
}

\begin{document}

\maketitle
\tableofcontents
\newpage
\printnoidxglossary
\newpage

\begin{abstract}
	Entwickler von \gls{igai} müssen sichergehen, dass ihre \Gls{igai} so wenig wie möglich unethischen Inhalt generiert.
	Dieses Paper analysiert Methoden, die Entwickler verwenden können, um dieses Ziel zu erreichen. 
	Um die Methoden zusammenzustellen, wurde zunächst auf ethische Vorgaben zu KI eingegangen und anschließend eine Literaturübersicht erstellt. 
	Fokus der Literaturübersicht war Trainingsmethoden und Möglichkeiten zur Verbesserung von AI allgemein und Möglichkeiten, Inhalte zu moderieren. 
	Als Ergebnis stellt dieses Paper ein Liste an Methoden bereit, die Entwickler verwenden können, um ihre IGAI bezüglich ethischer Aspekte zu verbessern. 
	Die Methoden können in verschiedenen Abschnitten der Entwicklung einer IGAI eingesetzt werden und haben unterschiedliche Komplexität. 
	Dieses Paper kann als Grundlage dienen, um ethisch korrekter handelnde AI zu entwickeln.
\end{abstract}


\chapter{Einleitung}
Die Generierung von Bildern mithilfe von AI reicht zurück bis in die 1960er Jahre\cite{Garcia}, doch erst in den letzten zwei Jahren wurden diese AIs durch die Veröffentlichungen von Projekten wie DALL-E (01/21), Midjourney (03/22) oder Stable Diffusion (08/22) der Allgemeinheit zugänglich gemacht. Auslotungen der Leistungsfähigkeit dieser AIs zeigen jedoch auf, dass sie sexistische oder rassistische Verzerrungen aufweisen \cite{Schmidt} und für unethische Zwecke missbraucht werden können, z. B. zur Erstellung von Deepfakes, pornografischen oder gewalttätigen Inhalten \cite{Hadero}.
Angesichts dessen haben u. a. die Entwickler von Midjourney reagiert und das Verwenden von Wörtern mit Bezug zur Thematik menschlicher Fortpflanzungssysteme verboten \cite{Heikkilae} und ihre kostenlose Testversion eingestellt \cite{NelsonMidjourney}. Daraus ergibt sich die Frage, welche Methodiken generell möglich sind bzw. angewandt werden, um die Erzeugung unethischer Inhalte durch IGAI zu verhindern.
Die vorliegende Arbeit setzt sich mit dieser Forschungsfrage auseinander, indem sie zusammenträgt, welche Methoden in der Literatur beschrieben werden, um AIs vor Missbrauch zu schützen. Sie diskutiert, wie die Anbieter und Entwickler dieser Technologie verantwortungsbewusst handeln können, um sicherzustellen, dass ihre AIs nur für ethisch vertretbare Zwecke eingesetzt werden. Das Ziel besteht darin, aufzuzeigen wie IGAI-Entwickler dazu beitragen können, dass mit ihrer Technologie keine unethischen Inhalte erzeugt werden.

% Bitte bei allen Zitationen an Seitenzahlen denken, also z. B., auf welcher Seite Schmidt das sagt. -> Das sind hier alles nur Nachrichtenartikel, haben keine Seitenzahl

\chapter{Definitionen}
Im Folgenden wird ein kurzer Abriss gegeben, wie sich der Begriff der Ethik bezüglich bildgenerierender AI in den letzten vier Jahren auf internationaler und nationaler Ebene entwickelt haben und welche Fachtermini aus der Thematik bildgenerierende AI in der vorliegenden Arbeit wie verwendet werden.
\section{Ethik}\label{def_ai}

\paragraph{UNESCO\cite{UNESCO}\\}
Im November 2019 verabschiedete die Generalversammlung der Organisation der Vereinten Nationen für Bildung, Wissenschaft und Kultur (\textbf{UNESCO}) auf ihrer 40. Sitzung die 40 C/Resolution 37, durch die sie die Generaldirektorin beauftragte, \glqq ein international gültiges Instrument zur Festlegung von Standards in Bezug auf die Ethik der künstlichen Intelligenz (AI) in Form einer Empfehlung vorzubereiten\glqq{}.\\Im November 2021 wurde auf der 41. Sitzung der \textbf{UNESCO} diese Empfehlung von den Mitgliedstaaten als weltweit erste globale Norm für die Ethik der künstlichen Intelligenz angenommen.\\

Diese Empfehlungen basieren auf allgemeingültigen Werten und Grundsätzen wie der Achtung der Menschenrechte, der Grundfreiheiten und der Rechtsstaatlichkeit in der digitalen Welt ebenso wie in der realen Welt. Das soll die Mitgliedstaaten ermutigen, diese Empfehlung freiwillig bei der Nutzung künstlicher Intelligenz einzuhalten und auch als Umsetzung ihres nationales Rechts zu begreifen. Im Detail werden folgende Werte in der Empfehlung angewandt:
%Zitation!

\paragraph{Werte:}
\begin{itemize}
	\item Respect, protection and promotion of human rights and fundamental freedoms and human dignity
	\item Environment and ecosystem flourishing
	\item Ensuring diversity and inclusiveness
	\item Living in peaceful, just and interconnected societies
\end{itemize}

Außerdem spiegeln sich diese Grundsätze wider:

\paragraph{Grundsätze:}
\begin{itemize}
	\item  Proportionality and Do No Harm
	\item  Safety and security
	\item  Fairness and non-discrimination
	\item  Sustainability
	\item  Right to Privacy, and Data Protection
	\item  Human oversight and determination
	\item  Transparency and explainability
	\item  Responsibility and accountability
	\item  Awareness and literacy
	\item  Multi-stakeholder and adaptive governance and collaboration
\end{itemize}
\paragraph{EU\cite{EUCommision}\\}
Im April 2019 veröffentlichte die Kommission der Europäischen Union \cite{EUCommision}.
% Bitte "(\textbf{EU}) \glqq ETHIK-LEITLINIEN FÜR EINE VERTRAUENSWÜRDIGE AI\glqq{}.\\" vollständig durch korrekte Zitation ersetzen, also so ähnlich wie "Im April 2019 veröffentlichte die Kommission der Europäischen Union [Kommission der Europäischen Union, 2019]". -> Hoffe das passt jetzt so
Demnach muss eine AI während ihres gesamten Lebenszyklus drei Komponenten erfüllen, um vertrauenswürdig zu sein: Sie muss a) rechtmäßig, b) ethisch und c) robust sein. Diese Komponenten bedienen die nachstehenden Grundsätze und Werte und geben ihre Umsetzung bei der Entwicklung, Einführung und Verwendung von AI-Systemen vor:

\paragraph{Werte:}
\begin{itemize}
	\item  Verhältnismäßigkeit und Schadensvermeidung
	\item  Sicherheit und Schutz
	\item  Fairness und Nichtdiskriminierung
	\item  Nachhaltigkeit
	\item  Recht auf Privatsphäre und Datenschutz
	\item  Menschliche Aufsicht und Entscheidungsbefugnis
	\item  Transparenz und Erklärbarkeit
	\item  Verantwortung und Rechenschaftspflicht
	\item  Bewusstsein und Bildung
	\item Multi-Stakeholder und adaptive Governance und Zusammenarbeit
\end{itemize}

\paragraph{Grundsätze:}
\begin{itemize}
	\item  Achtung der menschlichen Autonomie
	\item  Schadensverhütung
	\item  Fairness
	\item  Erklärbarkeit
\end{itemize}

Aufbauend auf diesen Grundsätzen wird vertrauenswürdige AI mithilfe eine Liste von sieben Anforderungen umgesetzt und verwirklicht, die erfüllt und durch sowohl technische und nicht-technische Ansätze während des gesamten Lebenszyklus eines AI-Systems umgesetzt werden müssen.

Die sieben Anforderungen lauten wie folgt:

\paragraph{Anforderungen an eine vertrauenswürdige AI:}
\begin{enumerate}
	\item  Vorrang menschlichen Handelns und menschliche Aufsicht
	\item  Technische Robustheit und Sicherheit
	\item  Schutz der Privatsphäre und Datenqualitätsmanagement
	\item  Transparenz
	\item  Vielfalt, Nichtdiskriminierung und Fairness
	\item  Gesellschaftliches und ökologisches Wohlergehen
	\item  Rechenschaftspflicht
\end{enumerate}

Seit April 2021 liegt ein Vorschlag der Europäischen Kommission vor, einheitliche Regeln für künstliche Intelligenz festzulegen, der einige Rechtssysteme in der Union ändern soll \cite{GesetzesentwurfEUComm}.

Vor dem Hintergrund dieses Vorschlags haben die Regierungen einiger Länder, wie die der Bundesrepublik Deutschland, versucht unter Berücksichtigung der Studie des Deutschen Komitees für die UNESCO in \cite[UNESCO-Empfehlung zur Ethik Künstlicher Intelligenz. Bedingungen zur Implementierung in Deutschland]{Kettemann} zu prüfen, ob in Bezug auf die diesbezüglichen Empfehlungen der UNESCO weiterer Handlungsbedarf besteht \cite{Deutscher_Bundestag}.  
Neben diesen Empfehlungen der Kommission der Europäischen Union gibt es von der Unesco definierte Empfehlungen \cite{UNESCO}. Diese werden in der vorliegenden Arbeit jedoch nicht betrachtet, und dies aus zwei Gründen: Einerseits überschneiden sich beide Empfehlungen, andererseits sind zwar beide nicht rechtlich bindend, die Empfehlungen der Kommission der Europäischen Union sollen jedoch im Gegensatz zu denen der UNESCO als Basis für ein für alle EU-Mitgliedsstaaten geltendes Gesetz dienen und kommen der rechtlichen Bindung damit am nächsten.

 

\section{AI}\label{def_ki}
Generative artifical intelligence (GAI) können neue Daten generieren, im Gegensatz zu vor deren Einführung entwickelten AIs (im Englischen AI), die nur Daten analysieren oder basierend auf gegebenen Daten zu handeln. Außerdem werden ihnen zum Trainieren enorme Datenmengen wie der komplette Inhalt von Wikipedia zur Verfügung gestellt. Daher sind diese AIs erst mit steigender Rechenkapazität möglich geworden. GAIs modellieren Netze aus miteinander verbundenen Daten in verschiedenen multimedialen Formaten. Demnach bestünden beispielsweise Verbindungen zwischen dem Wort Panda, ein Bild von einem Panda und einem Video von einem Panda. Dadurch können GAIs ein beliebiges Eingabeformat in ein beliebiges Ausgabeformat übersetzen. Thema dieser Arbeit sind image generating artifical intelligence (IGAI) oder auch text-to-image AIs, eine Unterkategorie von GAIs. Diese AIs können nach Eingabe von natürlichsprachlichem Text Bilder erzeugen.
\cite{Roberto}

%bitte hier nochmal über Konkretisierung von "steigender Rechenkapazität" nachdenken -> kommt so aus Quelle, laut Michel
% bitte nochmal überlegen, wie man beide schwammigen "bisher" auflösen kann -> kommt so aus Quelle, laut Michel
%bitte hier nochmal über Zitation o. ä. nachdenken -> kommt so aus Quelle, laut Michel

Um GAIs zu trainieren, werden drei Arten von Machine Learning verwendet: überwachtes Lernen, unüberwachtes Lernen und verstärktes Lernen. 
Beim überwachten Lernen werden gekennzeichnete Datensätze verwendet, um Vorhersagen zu treffen. Die gekennzeichneten Daten werden als Trainingsdaten verwendet und nach dem Ausführen des Algorithmus wird überprüft, ob die getroffene Vorhersage der Kennzeichnung entspricht.
Beim unüberwachten Lernen wird dem Algorithmus nicht gekennzeichnete Daten zugeführt. Hierbei wird nicht versucht eine Vorhersage zu treffen, sondern Schlussfolgerungen oder Zusammenhänge zwischen den Daten zu finden.
Beim verstärkten Lernen wird ein System aus Belohnung und Strafen verwendet. Macht der Algorithmus wenige Fehler ist die Belohnung groß und die Strafe klein, macht der Algorithmus viele Fehler ist die Belohnung klein und die Strafe groß. Dadurch kann das Finden des optimalen Verhaltens automatisiert werden.
\cite{serafeim}

\include{StandForschung.tex}
\include{Methodik.tex}

\chapter{Durchführung und Ergebnisse}\label{execution}
Verschiedene Formen der Voreingenommenheit von AI lassen sich durch unterschiedliche Methoden verhindern. Nachfolgend werden die Methoden erläutert, wobei sie in technische und nicht-technische unterteilt werden.

\section{Nicht-Technische Methoden}
Nicht-technische Methoden sind solche, die nicht primär auf technische Lösungen setzen.\\
Die hiernach beschriebenen Methoden wurden, wenn keine andere Quelle zitiert wird, aus \cite{UNESCO} und \cite{EUCommision} abgeleitet. %bitte schauen, ob das mit dem Zitierstil so passt, ich will einfach nur, dass da so etwas wie "...aus [Fitter, 2022] und [Kassem, 2023] abgeleitet" steht. -> wird so schon passen

\subsection{Datenverzerrungen im Trainingsdatensatz}
Bereits bei der Zusammenstellung des Trainingsdatensatzes einer AI kann es zu Verzerrungen kommen. Würde eine AI durch einen so verzerrten Datensatz trainiert werden, könnte sich bei der Anwendung der AI diese Verzerrung wiederspiegeln. Eine antrainierte Verzerrung lässt sich, wenn überhaupt, nur sehr aufwendig wieder korrigieren, es empfiehlt sich also, von vornherein einen geeigneten Datensatz zu verwenden.

Dabei wird zwischen Stichprobenverzerrungen, Messwertverzerrungen, Kennzeichnungsverzerrungen oder Verzerrung durch Einseitigkeit unterschieden \cite[S. 48ff.]{Srinivasan}.
Beispiele für Stichprobenverzerrungen können sein, dass zu wenig Daten in die Datensätze aufgenommen werden und diese keine realitätsnahe Darstellung ermöglichen.
Auch ist es möglich, dass die Daten systematisch einseitig sind. Ein Gesichtserkennungsmodell, das hauptsächlich an hellhäutigen Gesichtern trainiert wurde, kann z. B. bei dunkleren Hauttönen schlechter abschneiden oder bestimmte Gesichtsformen und -farben gar nicht als Gesichter erkennen.
Zudem ist eine systematisch Messwertverzerrung möglich, etwa wenn Bilder, die mit einer nicht vollständig funktionstüchtigen Kamera aufgenommen worden, in die Trainingsdaten einbezogen werden.
Es kann auch vorkommen, dass Daten falsch, überflüssig oder unzuverlässig gekennzeichnet werden. So ist die Unterscheidung zwischen \textit{Wiese} und \textit{Gras} selten nötig. Existieren allerdings beide Label im Trainingsdatensatz einer AI, kann sich dies negativ auf die Genauigkeit des Modells auswirken, wenn bei der Beschriftung nicht konsequent beide Bezeichnungen unterschieden werden\cite[S. 48ff.]{Srinivasan}. 

Eine Überwachungsinstanz in Form eines neutralen Beobachters oder Systems zuzüglich einer reflektierten Selbstkontrolle der Datensammler kann dabei helfen, solche Verzerrungen zu vermeiden.
Bei dieser Überwachung sollten mindestens nachstehende Überlegungen Beachtung finden: %Haben wir uns die ausgedacht oder müssen wir sie zitieren? -> ausgedacht
\begin{itemize}

    \item Repräsentativität: Die Daten müssen die Realität widerspiegeln und nicht nur Randgruppen oder Mehrheiten darstellen.
    \item Quantität: Die Anzahl der Daten muss ausreichen,  um eine Wiederspiegelung der Realität gewährleisten zu können.
    \item Qualität: Die Kennzeichnung der Daten muss auf Voreingenommenheit und Redundanz geprüft werden.
    \item Güte: Die Daten müssen qualitativ hochwertig, mindestens aber für den Anwendungsfall ausreichend sein.

\end{itemize}

\subsection{Verzerrungen in der Anforderungsformulierungen}
Bei den Definitionen einer Aufgabenstellung und der Anforderungen an ein Modell kann es zu Verzerrungen kommen. Diese Verzerrung kann einerseits durch die Formulierung der Aufgabenstellung aus den Anforderungen oder durch die Art der verwendeten Daten entstehen.  \cite[S. 51f.]{Srinivasan} 

Ein Problem bei der Formulierung der Anforderungen an einen Algorithmus kann beispielsweise darin bestehen, wenn Daten wie Geschlecht, Hautfarbe oder ethnische Zugehörigkeit in die Entscheidung einfließen, ob einer Person ein Kredit gewährt wird. Hierdurch können sexistische oder rassistische Diskriminierungen bei der Entscheidung stattfinden \cite[S. 51f.]{Srinivasan}.

Es empfiehlt sich das Einsetzten eines neutralen Beobachters, welcher die Anforderungen und daraus entstehende Aufgabenstellung auf folgende Aspekte prüft: %Haben wir uns die ausgedacht oder müssen wir sie zitieren? -> ausgedacht
\begin{itemize}
    \item Bei dem Treffen einer Entscheidung des Modells dürfen keine Aspekte berücksichtigt werden, die für eine erfolgreiche Ergebnisfindung der Aufgabenstellung nicht nötig sind.
    \item Die Formulierung muss neutral gestaltet sein, sodass sich durch die Formulierung an sich keine Schlüsse ziehen lassen, die mit Voreingenommenheit einhergehen.
\end{itemize}

\subsection{Vermeidung von Verzerrungen bei der Modellbewertung}
Neben der Möglichkeit, dass die Trainingsdaten oder der Algorithmus Verzerrungen enthalten, kann es auch sein, dass die Evaluation des Modells Verzerrungen aufweist.
Diese können durch menschliches Verhalten der Tester, oder durch schlecht entworfene Test-Frameworks auftreten \cite[S. 54f.]{Srinivasan}.

Menschliche Bewerter eines Modells können auf verschiedene Arten unethisch handeln. Sie können durch eigene Einstellungen und Erfahrungen beeinflusst sein, auch unbeabsichtigt.
Gerade wenn ein Bewerter direkt am Model beteiligt ist, kann es zum sogenannten Bestätigungsfehler, oder \glqq Peak-End-Effect\grqq{} kommen. Ein Bestätigungsfehler ergibt sich aus der Neigung, durch subjektive Wahrnehmung eine Erwartung bestätigt zu sehen. Ist so beispielsweise der Beurteiler beruflich oder finanziell direkt vom Erfolg des Projekts abhängig ist es eher möglich, dass Fehler nicht auffallen oder relativiert werden, als wenn er es nicht wäre.
Beim \glqq Peak-End-Effect\grqq{} wird eine Bewertung anhand von einzelnen Besonderheiten oder dem letzten Eindruck geprägt.
Ferner kann ein entworfenes Test-Framework fehlerhaft sein, wenn darin zum Beispiel zu wenig getestet wird, oder mit nicht geeigneten Testdaten gearbeitet wird \cite[S. 54f.]{Srinivasan}.

Um ein Modell ausreichend zu testen sollten einige Punkte berücksichtigt werden: %Haben wir uns die ausgedacht oder müssen wir sie zitieren? -> ausgedacht
\begin{itemize}
    \item Wenn Menschen das Modell bewerten, sollten diese nicht direkt an der Entwicklung beteiligt sein, um Neutralität zu gewährleisten.
    \item Wenn das Modell durch Menschen evaluiert wird, sollte diese Evalutaion von mehreren Personen mit unterschiedlichen Hintergründen durchgeführt werden.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses mindestens eine für den Anwendungsfall annehmbare Menge an voneinander unabhängigen Tests durchführen.
    \item Wird für die Evaluation ein Test-Framework verwendet, muss dieses weitreichend auf systematische Fehler überprüft werden.
\end{itemize}


\subsection{Abgrenzung des rechtlichen Rahmens durch {AGB}s}
Eine Etablierung von allgemeinen Geschäftsbedingungen (AGBs) legt den rechtlichen Rahmen für die Nutzung von IGAIs fest. Dadurch können sie dazu beitragen, den Missbrauch solcher Systeme einzudämmen. 
Allerdings sind AGBs allein nicht hinreichend, um den Missbrauch von IGAIs gänzlich zu verhindern, da sie keine aktive Nutzungsbeschränkung bieten. Wesentliche Aspekte könnten sein:
\begin{itemize}
    \item Festlegung des Nutzungszwecks: AGBs sollten explizit den beabsichtigten Nutzungszweck der AI definieren. Dadurch werden ein Rahmen für akzeptable Aktivitäten abgesteckt und unerwünschte Nutzungsformen ausgeschlossen.
    \item Untersagung unzulässiger Aktivitäten: AGBs sollten eine Liste von Aktivitäten enthalten, die als Missbrauch der AI gelten und somit untersagt sind.
    \item Haftungsausschluss: Ein Haftungsausschluss in den AGBs ist bedeutsam, um zu verdeutlichen, dass der Anbieter der AI nicht für den Missbrauch der Technologie durch die Nutzer verantwortlich gemacht werden kann. Dadurch sollen potenzielle rechtliche Konsequenzen für den Anbieter eingeschränkt werden.
    \item Festlegung von Nutzungsregeln: Die AGBs  können spezifische Regeln für die Nutzer der AI enthalten, um den Missbrauch einzudämmen. Dies könnte beispielsweise die Verpflichtung zur Registrierung, die Verwendung sicherer Zugangsdaten oder die Einhaltung ethischer Richtlinien umfassen.
    \item Überwachung und Sanktionen: AGBs sollten auch klare Informationen darüber bereitstellen, wie die Nutzung der AI überwacht wird und welche Sanktionen im Falle von Missbrauch verhängt werden können. Dies kann beispielsweise die Möglichkeit zur Überprüfung von Nutzeraktivitäten, die Sperrung von Konten oder die Meldung rechtswidrigen Verhaltens umfassen.
\end{itemize}

\subsection{Zusammenarbeit und Feedback}
Nutzerfeedback und Engagement beinhalten das aktive Einbeziehen von Nutzern während des Entwicklungsprozesses und die Integration ihrer Perspektiven, Bedenken und Werte in die Gestaltung und Entscheidungsfindung von AI-Systemen. Durch die Berücksichtigung des Nutzerfeedbacks können AI-Entwickler vielseitigere Einblicke in die potenziellen ethischen Implikationen ihrer Bildgenerierungsmodelle gewinnen und sicherstellen, dass die generierten Bilder mit den Erwartungen und Werten der Nutzer übereinstimmen.

Durch eine aktive Zusammenarbeit mit externen Interessengruppen, Forschern und der breiteren Gemeinschaft können Organisationen tiefgründigere Erkenntnisse sammeln, Bedenken ansprechen und ihre Praktiken kontinuierlich verbessern. Dieser kollaborative Ansatz gewährleistet, dass die Entwicklung und Bereitstellung von IGAI-Technologien mit gesellschaftlichen Werten und ethischen Standards im Einklang stehen.\cite{EUCommision}\cite{UNESCO}

\subsubsection{Partnerschaften über Sektorengrenzen hinweg}
Zusammenarbeit und Partnerschaften mit anderen Institutionen einschließlich akademischen Forschern, gemeinnützigen Organisationen, Regierungsbehörden und Branchenkollegen ermöglichen eine breitere Perspektive und ein umfassenderes Verständnis der ethischen Implikationen der Bildgenerierung mittels AI. Durch die Nutzung des gemeinsamen Fachwissens und der Ressourcen verschiedener Sektoren können Organisationen komplexe Herausforderungen angehen und einen ganzheitlicheren Ansatz für ethische AI sicherstellen.\cite{Vogel}

\subsubsection{Partizipative Gestaltung}
Indem die Endnutzer und Stakeholder in den Design- und Entwicklungsprozess von IGAI einbezogen werden, können die Entwickler nach \cite{Zytko} vielseitigere Einblicke gewinnen, spezifische Bedürfnisse identifizieren und Lösungen gemeinsam entwickeln, die den Werten der Nutzer und ethischen Standards entsprechen. Partizipative Designmethoden wie nutzerzentriertes Design und Co-Creation-Workshops fördern die Zusammenarbeit und Empathie und stellen sicher, dass AI-Systeme unter Berücksichtigung der Nutzerinteressen gestaltet werden.

\subsubsection{Öffentliche Beteiligung und Bewusstseinsbildung}
Laut \cite{WILSON2022101652} kann eine Förderung der öffentlichen Beteiligung an und Bewusstseinsbildung zu ethischen Aspekten bei der Anwendung von AI dienlich sein, um eine breitere Palette von Interessengruppen in den Dialog über die Ethik von Bildgenerierung durch AI einzubeziehen. Im Detail werden öffentliche Konsultationen, Workshops und öffentliche Debatten über die ethischen Implikationen von AI-Technologien empfohlen. Solche Initiativen schaffen \cite{WILSON2022101652} zufolge Raum für Diskussionen, fördern diverse Perspektiven und befähigen Einzelpersonen, zur Entwicklung ethischer Leitlinien und Richtlinien beizutragen.

\subsubsection{Kontinuierliche Überwachung und Evaluation}
Potenzielle Voreingenommenheit, unbeabsichtigte Folgen oder aufkommende ethische Fragen können gemäß \cite{EUCommision} durch die Etablierung fortlaufender Überwachungs- und Evaluierungsprozesse zur Bewertung der ethischen Auswirkungen und Leistung von IGAI-Systemen identifiziert werden. Dies bedeute konkret eine regelmäßige Erfassung und Analyse von Rückmeldungen von Nutzern, betroffenen Gemeinschaften und anderen Interessengruppen. Eine solche Feedbackschleife ermögliche es Organisationen, Bedenken proaktiv anzugehen, ihre Modelle zu überarbeiten und sicherzustellen, dass ethische Praktiken während des gesamten Lebenszyklus von IGAI-Systemen eingehalten werden.

\subsubsection{Nutzerfeedback}
In \cite[The ethics of algorithms: Mapping the debate]{Mittelstadt} wird argumentiert, dass Nutzerfeedback und -beteiligung zu mehreren ethischen Dimensionen von AI-Algorithmen beitragen können, einschließlich Transparenz, Verantwortlichkeit und Fairness. Dieses Konzept könnte um folgende Aspekte erweitert werden:

%muss das Folgende nicht als Aufzähungsliste formatiert werden? -> ist jetzt ne aufzählung
\begin{itemize}
	\item Transparenz: Nutzerfeedback kann dazu beitragen, die Transparenz von IGAI-Systemen zu erhöhen. Indem Nutzer in den Entwicklungsprozess einbezogen werden, können AI-Entwickler Informationen über die Funktion des Systems, seine Grenzen und die mögliche Voreingenommenheit teilen, die es haben kann. Transparenz ermöglicht den Nutzern ein besseres Verständnis dafür, wie ihre Daten verwendet werden und wie das AI-System Bilder generiert.
	\item Verantwortlichkeit: Nutzerfeedback spielt eine entscheidende Rolle bei der Rechenschaftspflicht von AI-Entwicklern und -Systemen. Es ermöglicht den Nutzern, Bedenken hinsichtlich potenzieller ethischer Probleme, Voreingenommenheit oder unbeabsichtigter Konsequenzen im Zusammenhang mit den generierten Bildern zu äußern. Entwickler können dieses Feedback nutzen, um Mängel zu identifizieren und zu beheben, die Algorithmen zu verfeinern und die Verantwortung für die Auswirkungen ihrer AI-Systeme zu übernehmen.
	\item Fairness: Nutzerfeedback hilft dabei, potenzielle Voreingenommenheit im IGAI-Prozess zu identifizieren. Unterschiedliche Nutzer können unterschiedliche Erwartungen, kulturelle Kontexte oder Sensibilitäten haben. Indem sie mit einer vielfältigen Nutzergruppe interagieren, können Entwickler Voreingenommenheit erkennen und mindern, die zu unfairer oder diskriminierender Bildgenerierung führen könnten. Nutzerfeedback bietet die Möglichkeit sicherzustellen, dass das AI-System eine breite Palette von Perspektiven berücksichtigt und verschiedene kulturelle und gesellschaftliche Normen respektiert.
\end{itemize}


\section{Technische Methoden}

Technische Ansätze können von den Entwicklern einer IGAI sowohl während als auch nach der Entwicklungsphase eingesetzt werden, beispielsweise in Form von Datenaufbereitung und -manipulation oder Filtern. Aufgrund dessen werden in der weiteren Durchführung diese Ansätze danach gruppiert, ob diese in der Entwicklungsphase der IGAI oder in deren Anwendungsphase anzuwenden sind.

\subsection{Entwicklungsphase}

\subsubsection{Verwendung von Datenerweiterung (Data Augmentation)}
Datenerweiterung beschreibt eine Technik, Varianz in vorhandene Daten einzufügen und so die Datenmenge zu erweitern. Dabei werden synthetische Daten aus vorhandenen Daten erzeugt, die geringe Unterschiede zu den Originaldaten haben, oder aus Kombinationen mehrerer Originaldaten entstehen \cite[S. 2]{Shorten}. %Falls Shorten nur diese Erzeugung synthetischer Daten erklärt, stimmt das so, wenn er sich allerdings die konkrete, in den folgenden Sätzen beschriebene Methode ausgedacht hat, bitte wie in den oberen Abschnitten einflechten, sodass klar ist, dass alles von ihm stammt. -> stammt von Dominics Kopf, der ist wenn Schuld
Im Fall von IGAIs könnten so Eingabetexte, mit denen ethisch kritische Inhalte erzeugt werden könnten, in mehreren leicht oder stark abgewandelten Formen der IGAI als Input-Text 
gegeben und anschließend die erzeugten Bilder überprüft werden, um die Konsistenz dieser zu überprüfen und einen besseren Umgang mit kritischen Inhalten anzutrainieren.  

\subsubsection{Verwendung von überwachtem Lernen}\label{Supervised Learning}
Nach dem im Kapitel \nameref{def_ai} vorgestellten Prinzip des überwachten Lernens könnte man für IGAIs Datensätze erstellen, welche aus 2-Tupeln bestehen: einem Text anhand dem ein Bild erzeugt werden soll und einem dazu passenden ethisch unbedenklichen Bild. Das Modell kann so anhand von positiven Beispielen lernen, welche Inhalte es erzeugen darf.

Auch das Gegenteil wäre möglich: ein geordnetes Paar an Texten und ethisch verwerflichen Bildern, anhand derer die AI lernen kann, was sie auf keinen Fall generieren darf.

\subsubsection{Verwendung von verstärktem Lernen}
Wie in \nameref{def_ai} erklärt, wird beim verstärkten Lernen ein Belohnungs-Bestrafungs-System eingesetzt. Dieses könnte im Fall von IGAIs daraus bestehen, die IGAI zu belohnen, wenn 
\begin{itemize}
    \item ethisch nicht vetretbare Texte erkannt werden und kein Bild erzeugt wird,
    \item ethisch vetretbare Texte erkannt werden und ein Bild erzeugt wird
\end{itemize}
oder zu bestrafen, wenn
\begin{itemize}
    \item ethisch nicht vetretbare Texte nicht erkannt werden und ein Bild erzeugt wird,
    \item ethisch vetretbare Texte erkannt werden, das erzeugte Bild aber nicht ethisch vetretbar ist.
\end{itemize}  

\subsubsection{Verwendung von kontradiktorischem Lernen (Adversarial Learning)}
Ein Machine-Learning-Algorithmus kann durch Anwendung von kontradiktorischem Lernen ethisch hochwertiger gestaltet werden. Dabei wird ein als Adversary bezeichneter zusätzlicher Algorithmus verwendet, um den Hauptalgorithmus herauszufordern und zu verbessern. \cite[S. 3]{Kurakin} Der Adversary wird darauf trainiert, gezielt ethisch problematische Beispiele zu generieren, die der Hauptalgorithmus möglicherweise falsch klassifizieren könnte. Durch die Konfrontation des Hauptalgorithmus mit solchen herausfordernden Beispielen wird dieser dazu gezwungen, seine Entscheidungen präziser zu treffen und ethisch akzeptablere Ergebnisse zu erzeugen.

\subsection{Anwendungsphase}

Die Anwendungsphase ist der Zeitraum von der Fertigstellung einer IGAI bis zu dem Zeitpunkt, an dem ihr Dienst eingestellt wird. In diesem Zeitraum wird die IGAI von den Nutzern verwendet. Techniken in der Anwendungsphase nehmen keinen direkten Einfluss auf die Funktionsweise der IGAI selbst. Die Techniken verändern die Rahmenbedingungen in denen die IGAI verwendet wird.

\subsubsection{Input-Filter}
Die Entwickler von IGAIs müssen davon ausgehen, dass Nutzer unethische Eingaben vornehmen. Damit ist der Input in Textform gemeint, den die IGAI verwendet, um daraus Bilder zu generieren. Um zu verhindern, dass diese Eingaben vorgenommen werden, können die Entwickler Input-Filter verwenden. In \cite[Artificial Intelligence as a Service for Immoral ContentDetection and Eradication]{Shah} wird eine AI beschrieben, die unmoralischen Inhalt aus sozialen Netzwerken erkennen kann. Die Entwickler von IGAIs können anhand dieser Ergebnisse ihre eigene AI entwickeln, jedoch mit der Spezialisierung auf unethischen Inhalt anstatt unmoralischen Inhaltes. Diese AI können sie dann als Filter verwenden, um einzuschränken welche Eingaben überhaupt zur IGAI gelangen.

\subsubsection{Output-Filter}
Wenn eine IGAI Bilder generiert, müssen Entwickler davon ausgehen, dass dabei unethische Bilder generiert werden können. Diese Bilder dürfen den Nutzer nicht erreichen. Um das zu verhindern können die Entwickler Output-Filter verwenden. Der Output ist das von der IGAI generierte Bild. \cite{Zheng} beschreibt ein System, das aus zwei getrennten Filtern besteht: einem Filter für verbotene Symbole und einem für nicht jugendfreie Bilder. Da diese Filter unabhängig voneinander arbeiten, kann das System um weitere Filter ergänzt werden. Dies ermöglicht eine Filterung aller Inhalte, die einem Nutzer der AI nicht ausgegeben werden sollen. Das beschriebene System ist laut \cite{Zheng} schneller und zuverlässig als seine Vorgänger. In Anbetracht des Alters des Artikels ist anzunehmen, dass heutige Systeme noch schneller und zuverlässiger den zu filternden Inhalt ermitteln. Ein solches System kann von den Entwicklern verwendet werden, um den Output zu überwachen. Erkennt das System ein generiertes Bild als unethisch wird es nicht an den Nutzer ausgegeben. Außerdem können diese Ergebnisse auch zur Weiterentwicklung der IGAI selbst verwendet werden.

\subsubsection{Anwendungsbeschränkung} %Haben wir uns das selbst ausgedacht oder ist das alles von Jang? Ich habe es mal so geschrieben, als käme alles von Jang, wenn nicht, bitte wieder zurückändern. -> Eigenes Beipspiel anhand von Jang, brauch also keine Quelle
Anwendungsbeschränkung bedeutet, dass Entwickler die Art und Weise wie Nutzer mit der IGAI interagieren einschränken. In dieser Methode soll es um eine Beschränkung der Anwendungsfrequenz gehen, wie sie \cite{Jang} beschreibt. Als Beispiel dient hierbei das Credit-System von DALL E2 von OpenAI. Bei diesem System werden Credits benötigt, um mittels der IGAI ein Bild zu generieren. Credits sind käuflich oder werden gestatten, wenn man sich in einem bestimmten Zeitraum nach Veröffentlichung von DALL E2 angemeldet hat. Wenn die IGAI ein Bild generiert, wird ein Credit verbraucht. Hat ein Nutzer keine Credits mehr, kann er keine Bilder mehr generieren. Dadurch wird eine initiale Barriere geschaffen, da Nutzer Credits kaufen müssen, um Bilder zu generieren. Nutzer, die auf die Generierung von unethischen Bildern abzielen, können damit abgeschreckt werden. Durch Anzahl der Credits kann zusätzlich beschränkt werden wie viele Bilder generiert werden können. Entwickler können dieses Credit-System nach ihrem Bedarf anpassen.


\chapter{Fazit}
Das Ziel der vorliegenden Arbeit besteht darin, die Frage zu beantworten, welche Methoden es aktuell gibt, mit denen IGAI-Entwickler dazu beitragen können, dass mit ihrer Technologie keine unethischen Inhalte erzeugt werden. Diese Frage konnte durch die detaillierte Auflistung und Erklärung der Methoden in Kapitel \nameref{execution} beantwortet werden. Die gewählte Methode, ausschließlich einen Überblick über die in der Literatur beschriebenen Methoden auszuführen, kann jedoch nicht zeigen, welche Methoden tatsächlich wie angewandt werden und ihren Zweck inwiefern erfüllen. Von den Autoren der vorliegenden Arbeit selbst übertragene Methoden wie die Verwendung von überwachtem Lernen in Kapitel \nameref{Supervised Learning} konnten nicht auf ihre tatsächliche Anwendbarkeit überprüft werden, da dies den Rahmen der vorliegenden Arbeit sprengen würde. Es würde sich im Anschluss an die vorliegende Arbeit also anbieten, experimentell die tatsächliche Umsetzung der genannten Methoden an verschiedenen IGAIs zu überprüfen bzw. auch experimentell zu untersuchen, inwiefern die skizzierten im Rahmen der vorliegenden Arbeit erdachten Methoden tatsächlich anwendbar und dienlich sind.
So kann diese Arbeit jedoch als Grundlage dienen, um einen Überblick über mögliche Methoden zu gewinnen, um darauf fußend ethisch korrekter handelnde AI zu entwickeln.

% Gefahr Achtung! Supervised Learning also überwachtes Lernen ist nicht selbst erdacht, ich hab nur die Quelle vergessen! -> habs zu übertragen geändert

\chapter{AI-Werkzeuge zur Unterstützung des wissenschaftlichen Arbeitens}
Für die Recherche wurden verschiedene AI-Werkzeuge verwendet, deren Nutzung im Folgenden eingeschätzt und die Nützlichkeit dessen diskutiert werden soll. 
ChatGPT3 ist ein Sprachmodell und dient somit der Fomulierung sprachlich korrekter Texte, kann jedoch beispielsweise keine Paper zusammenfassen, sondern gibt nur anhand des Titels jeweils ähnlich klingende Zusammenfassungen, ohne sich dabei auf die tatsächlichen Inhalte des Papers berufen zu können. Somit eignet sich höchstens, um die betrachtete Disziplin erforschende Autoren zu finden und auf dieser Basis mit anderen Werkzeugen weiterzurecherchieren. Solche Unterstützung liefern allerdings auch wissenschaftlich
etabliertere Webseiten wie WikiCFP. Zudem könnte ChatGPT3 von Wissenschaftlern, die in der Sprache, in der sie eine wissenschaftliche Arbeit schreiben, nicht ausreichend versiert sind, als Unterstützung bei der Formulierung eines wohlklingenden Texts verwendet werden.
Neben dem reinen ChatGPT3 wurde Bing mit ChatGPT4 verwendet. Dies bietet gegenüber der oben beschriebenen Variante den Vorteil, dass Bing das Internet durchsucht und somit inhaltlich passendere Antworten liefern kann. Der Nachteil besteht darin, dass nur das Internet beipielsweise nach dem Papertitel und „summary“ durchsucht wird und eines der ersten Suchergebnisse in wohlformulierter Sprache widergegeben wird, weshalb auch hier keine wirklich brauchbaren Inhalte erzeugt werden.
Generell muss die Benutzung eines Werkzeugs geübt werden, um die Möglichkeiten dessen in guter Qualität auszuschöpfen. Demzufolge  müsste länger und intensiver mit diesem Werkzeug gearbeitet werden, um eventuell nützlichere Ergebnisse zu erzielen als diese. Die oben genannte Einschätzung beruht nur auf einwöchige Nutzung ohne nennenswerte Vorkenntnisse und ist demnach nicht repräsentativ für das tatsächliche Potenzial der genutzten AI-Werkzeuge.
Dasselbe gilt für Elicit. Dieses Werkzeug ist durch die Größe seiner Datenbank beschränkt, innerhalb dieser jedoch hilfreich, um einen Überblick über Werke zu erlangen, deren Autoren nicht auf den hiesigen Konferenzen vertreten und folglich mithilfe der klassischen Recherche über Konferenzen und deren Paper und Teilnehmer nicht auffindbar wären. Mithilfe geeigneter Fragen kann auch Zeit bei der Recherche gespart werden, indem die Zusammenfassung der ersten vier Paper genutzt wird. Diese sollte jedoch nicht nur, wenn die eigentlich intendierte Frage nicht direkt beantwortet wird, überprüft werden, da sie sehr kurz und nicht immer vollständig zutreffend ist.

\bibliographystyle{ieeetr}
\bibliography{Sources.bib}
\end{document}